{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63f5d3d4",
   "metadata": {},
   "source": [
    "hw9 - rohan bhatt - msml604"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fc4a17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Method  Iter    x0   x1     ‖x - x*‖  f(x_final)\n",
      "             GD (a=0.02)  2436 -5.95 6.05 1.997032e-10      -18.05\n",
      "Momentum (n=0.02, p=0.5)  1188 -5.95 6.05 1.992906e-10      -18.05\n",
      "Nesterov (n=0.02, p=0.5)  5000   NaN  NaN          NaN         NaN\n",
      "                 RMSprop  5000 -6.00 6.00 7.071068e-02      -18.00\n",
      "                    Adam   216 -5.95 6.05 1.932902e-10      -18.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-6a7615e84908>:16: RuntimeWarning: overflow encountered in matmul\n",
      "  return A @ x + b\n",
      "<ipython-input-5-6a7615e84908>:70: RuntimeWarning: invalid value encountered in add\n",
      "  v = rho * v + g #same momentum update as before\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.linalg import norm, solve\n",
    "\n",
    "#problem data (f(x) = 0.5 * x^T A x + b^T x)\n",
    "A = np.array([[10.25, 9.75],\n",
    "              [9.75, 10.25]])\n",
    "b = np.array([2.0, -4.0])\n",
    "\n",
    "#objective function\n",
    "def f(x):\n",
    "    return 0.5 * x @ A @ x + b @ x\n",
    "\n",
    "#gradient\n",
    "def grad_f(x):\n",
    "    return A @ x + b\n",
    "\n",
    "#x* and f*, ground truth sol\n",
    "x_star = -solve(A, b) # x* = -A^-1\n",
    "f_star = f(x_star) # f(x*)\n",
    "\n",
    "#optimization loop settings\n",
    "x0 = np.zeros(2) #starting point (0,0)\n",
    "max_iterations = 5000 #hard iteration cap\n",
    "tol = 1e-10 #stop when ||f|| < tol\n",
    "results = [] #list of dicts, will turn into DF later\n",
    "\n",
    "#helper to log each optimiser's outcome\n",
    "def store(name, iters, x_final):\n",
    "    results.append({\n",
    "        \"Method\": name,\n",
    "        \"Iter\": iters,  \n",
    "        \"x0\": x_final[0], #first coordinate\n",
    "        \"x1\": x_final[1], #second coordinate\n",
    "        \"‖x - x*‖\": norm(x_final - x_star), #distance to optimum\n",
    "        \"f(x_final)\": f(x_final) #objective val reached\n",
    "    })\n",
    "\n",
    "# steepest descent with fixed step size\n",
    "alpha = 0.02\n",
    "x = x0.copy()\n",
    "for k in range(1, max_iterations + 1):\n",
    "    g = grad_f(x) #current gradient\n",
    "    if norm(g) < tol:  #convergnce test\n",
    "        break\n",
    "    x -= alpha * g #gradient step\n",
    "store(\"GD (a=0.02)\", k, x)\n",
    "\n",
    "# gradient with momentum (heavy ball)\n",
    "rho = 0.5 #momentum\n",
    "eta = .02 #learning rate n\n",
    "x = x0.copy()\n",
    "v = np.zeros_like(x) #momentum buffer\n",
    "for k in range(1, max_iterations + 1):\n",
    "    g = grad_f(x)\n",
    "    if norm(g) < tol:\n",
    "        break\n",
    "    v = rho * v + g #accumulate gradient into momentum\n",
    "    x -= eta * v #update with momentum term\n",
    "store(\"Momentum (n=0.02, p=0.5)\", k, x)\n",
    "\n",
    "# Nesterov accelerated gradient (look ahead)\n",
    "eta, rho = 0.02, 0.5\n",
    "x = x0.copy()\n",
    "v = np.zeros_like(x)\n",
    "for k in range(1, max_iterations + 1):\n",
    "    g = grad_f(x - rho * v)\n",
    "    if norm(g) < tol:\n",
    "        break\n",
    "    v = rho * v + g #same momentum update as before\n",
    "    x -= eta * v\n",
    "store(\"Nesterov (n=0.02, p=0.5)\", k, x)\n",
    "\n",
    "# RMSprop \n",
    "eta = .15 #learning rate\n",
    "eps = .5 #stabiliser e\n",
    "rho_rms = .8 #decay rate\n",
    "x = x0.copy()\n",
    "s = np.zeros_like(x) #running avg of squared grads\n",
    "for k in range(1, max_iterations + 1):\n",
    "    g = grad_f(x)\n",
    "    if norm(g) < tol:\n",
    "        break\n",
    "    s = rho_rms * s + (1 - rho_rms) * g**2 #update moving 2nd moment\n",
    "    x -= eta * g / (np.sqrt(s) + eps) #divide by RMS magnitude\n",
    "store(\"RMSprop\", k, x)\n",
    "\n",
    "# Adam ( momentum + RMSprop + bias correction)\n",
    "eta, eps, beta1, beta2 = 0.15, 0.5, 0.8, 0.8\n",
    "eta = 0.15 #learning rate\n",
    "eps = 0.5 #stabiliser e\n",
    "beta1 = 0.8 #momentum decay rate\n",
    "beta2 = 0.8 #RMS decay rate\n",
    "x = x0.copy()\n",
    "m = np.zeros_like(x) #1st moment (mean grad)\n",
    "v = np.zeros_like(x) #2nd moment (squared grad)\n",
    "for k in range(1, max_iterations + 1):\n",
    "    g = grad_f(x)\n",
    "    if norm(g) < tol:\n",
    "        break\n",
    "    m = beta1 * m + (1 - beta1) * g #EWMA of gradient\n",
    "    v = beta2 * v + (1 - beta2) * g**2 #EWMA of squared gradient\n",
    "    m_hat = m / (1 - beta1**k) #bias-corrected 1st moment\n",
    "    v_hat = v / (1 - beta2**k) #bias-corrected 2nd moment\n",
    "    x -= eta * m_hat / (np.sqrt(v_hat) + eps) #adam update\n",
    "store(\"Adam\", k, x)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6b6833",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
