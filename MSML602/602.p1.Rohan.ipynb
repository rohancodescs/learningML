{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rohan Bhatt\n",
    "#### UID: 117942330"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Research Question: Are bank failures more closely correlated with general economic recessions or with specific financial trends (e.g., Crypto boom, ML boom)?\n",
    "##### This question was interesting to me because I have worked in the fintech space for several of my internships, and since I was 18 I've always had an interest in the stock market (I learned to trade, and subsequently fail/break-even trading options), so given my general interest in the space, seeing the fall of FTX, SVB, and more sparked my interest. Were there failures like FTX with an industry during the .COM boom? Or do banks usually sell out during recessions? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - The population we will be working with is all banks in the United States, with a focus on those who have failed since October 1st, 2000.\n",
    "##### - Variables: Dependent variable: Bank Failure (binary, fail or no fail). Independent variables are many, including economic indicators like GDP growth rate, unemployment. inflation, etc as well as financial trend indicators like capitalization of certain markets i.e. crypto, investment in industries like AI, etc. Potentially confounding variables include bank size (total assets), geographic location, and regulatory changes\n",
    "##### - Hypothesis: Bank failures are more strongly correlated with general economic recessions than with specific financial trends, its just that financial trends have exacerbated the risk of failure during economic pitfalls/downturns.\n",
    "##### - Data collection: The primary dataset is the FDIC failed bank list, economic data collected will be historical data on GDP growth rates, unemployment rates, and other relative economic indicators from places like the Federal Reserve, Census Bureau, etc. Financial trend data will also be important like historical data on crypto market capitalization, and capitalization of specific markets over a 20 year period (10/1/2000-2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pyspark matplotlib #ONLY RUN ON COLAB\n",
    "\n",
    "#ONLY RUN LOCALLY\n",
    "import findspark\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "findspark.init()\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--verbose pyspark-shell'\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+------+-----+----------------------+-------------+-----+\n",
      "|          Bank Name�|        City�|State�|Cert�|Acquiring Institution�|Closing Date�| Fund|\n",
      "+--------------------+-------------+------+-----+----------------------+-------------+-----+\n",
      "|Republic First Ba...| Philadelphia|    PA|27332|  Fulton Bank, Nati...|    26-Apr-24|10546|\n",
      "|       Citizens Bank|     Sac City|    IA| 8758|  Iowa Trust & Savi...|     3-Nov-23|10545|\n",
      "|Heartland Tri-Sta...|      Elkhart|    KS|25851|  Dream First Bank,...|    28-Jul-23|10544|\n",
      "| First Republic Bank|San Francisco|    CA|59017|  JPMorgan Chase Ba...|     1-May-23|10543|\n",
      "|      Signature Bank|     New York|    NY|57053|   Flagstar Bank, N.A.|    12-Mar-23|10540|\n",
      "+--------------------+-------------+------+-----+----------------------+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- Bank Name�: string (nullable = true)\n",
      " |-- City�: string (nullable = true)\n",
      " |-- State�: string (nullable = true)\n",
      " |-- Cert�: integer (nullable = true)\n",
      " |-- Acquiring Institution�: string (nullable = true)\n",
      " |-- Closing Date�: string (nullable = true)\n",
      " |-- Fund: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pyspark.sql.functions import to_date, col, countDistinct, isnan, when, count, year, reduce, regexp_replace, col\n",
    "import matplotlib.pyplot as plt\n",
    "df = spark.read.csv(\"banklist.csv\", header=True, inferSchema=True)\n",
    "\n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-----+-----+---------------------+------------+-----+\n",
      "|           Bank Name|         City|State| Cert|Acquiring Institution|Closing Date| Fund|\n",
      "+--------------------+-------------+-----+-----+---------------------+------------+-----+\n",
      "|Republic First Ba...| Philadelphia|   PA|27332| Fulton Bank, Nati...|   26-Apr-24|10546|\n",
      "|       Citizens Bank|     Sac City|   IA| 8758| Iowa Trust & Savi...|    3-Nov-23|10545|\n",
      "|Heartland Tri-Sta...|      Elkhart|   KS|25851| Dream First Bank,...|   28-Jul-23|10544|\n",
      "| First Republic Bank|San Francisco|   CA|59017| JPMorgan Chase Ba...|    1-May-23|10543|\n",
      "|      Signature Bank|     New York|   NY|57053|  Flagstar Bank, N.A.|   12-Mar-23|10540|\n",
      "+--------------------+-------------+-----+-----+---------------------+------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Trying to get rid of weird symbols and cleaning up data\n",
    "new_column_names = [re.sub(r'[^\\x00-\\x7F]', '', col_name) for col_name in df.columns]\n",
    "df = df.toDF(*new_column_names)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First steps will be to do time-series on the dates, and see commonalities (i.e. does anything hover more over 2008? from there will import more datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2:\n",
    "#### Preliminary info / looking for specific needs in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data:\n",
      "Number of rows: 569\n",
      "Number of columns:  7\n",
      "Data Schema: \n",
      "root\n",
      " |-- Bank Name: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Cert: integer (nullable = true)\n",
      " |-- Acquiring Institution: string (nullable = true)\n",
      " |-- Closing Date: string (nullable = true)\n",
      " |-- Fund: integer (nullable = true)\n",
      "\n",
      "List of columns: \n",
      "Bank Name\n",
      "City\n",
      "State\n",
      "Cert\n",
      "Acquiring Institution\n",
      "Closing Date\n",
      "Fund\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of data:\")\n",
    "\n",
    "print(f\"Number of rows: {df.count()}\")\n",
    "print(f\"Number of columns:  {len(df.columns)}\")\n",
    "\n",
    "print(\"Data Schema: \")\n",
    "df.printSchema()\n",
    "\n",
    "print(\"List of columns: \")\n",
    "for column in df.columns: print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Descriptive Statistics\n",
      "+-------+--------------------+-------+-----+------------------+---------------------+------------+------------------+\n",
      "|summary|           Bank Name|   City|State|              Cert|Acquiring Institution|Closing Date|              Fund|\n",
      "+-------+--------------------+-------+-----+------------------+---------------------+------------+------------------+\n",
      "|  count|                 569|    569|  569|               569|                  569|         569|               569|\n",
      "|   mean|                NULL|   NULL| NULL|31653.056239015816|                 NULL|        NULL|10042.210896309314|\n",
      "| stddev|                NULL|   NULL| NULL|16464.868899588426|                 NULL|        NULL|1110.6370829809036|\n",
      "|    min|1st American Stat...|Acworth|   AL|                91|      1st United Bank|    1-Aug-08|              4645|\n",
      "|    max|               ebank|Wyoming|   WY|             59017|  Your Community Bank|    9-Sep-11|             10546|\n",
      "+-------+--------------------+-------+-----+------------------+---------------------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Descriptive Statistics\")\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This makes sense; NULL is going to be shown for all the string variables, and there isn't necesssarily math involved in this. Although df.describe() gets the mean, stdev, etc, it doesn't make sense to find the mean and stdev for the cert and fund as those are just identifiers. Each variable count being 569 shows consistency; there aren't any missing values and its complete. There are 569 total number of bank failures. Below we can go analyze the number of unique entries per column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " looking for unique entries in columns\n",
      "Bank Name: 551\n",
      "City: 436\n",
      "State: 44\n",
      "Cert: 569\n",
      "Acquiring Institution: 303\n",
      "Closing Date: 264\n",
      "Fund: 569\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n looking for unique entries in columns\")\n",
    "for column in df.columns:\n",
    "    unique_count = df.select(countDistinct(col(column))).collect()[0][0]\n",
    "    print(f\"{column}: {unique_count}\")\n",
    "print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is super useful as there were only 303 acquiring institutions, so it might be worth into looking later on the most common acquirer (i.e. JP Morgan, bigger Asian financial conglomerates). Below I am going to continue exploring this data set for any specific needs and other issues:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for non-numeric values in cert & fund columns\n",
      "+----------+-----+\n",
      "|Cert_check|count|\n",
      "+----------+-----+\n",
      "|   numeric|  569|\n",
      "+----------+-----+\n",
      "\n",
      "+----------+-----+\n",
      "|Fund_check|count|\n",
      "+----------+-----+\n",
      "|   numeric|  569|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#doing type checks on the cert and fund columns to ensure type consistency\n",
    "\n",
    "print(\"Checking for non-numeric values in cert & fund columns\")\n",
    "\n",
    "df.select(when(col(\"Cert\").cast(\"int\").isNull(), \"non-numeric\").otherwise(\"numeric\").alias(\"Cert_check\")).groupBy(\"Cert_check\").count().show()\n",
    "df.select(when(col(\"Fund\").cast(\"int\").isNull(), \"non-numeric\").otherwise(\"numeric\").alias(\"Fund_check\")).groupBy(\"Fund_check\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Bank failures by year: \n",
      "+----+-----+\n",
      "|Year|count|\n",
      "+----+-----+\n",
      "|NULL|  569|\n",
      "+----+-----+\n",
      "\n",
      "\n",
      "Top 10 states with most bank failures: \n",
      "+-----+-----+\n",
      "|State|count|\n",
      "+-----+-----+\n",
      "|   GA|   93|\n",
      "|   FL|   76|\n",
      "|   IL|   69|\n",
      "|   CA|   43|\n",
      "|   MN|   23|\n",
      "|   WA|   19|\n",
      "|   AZ|   16|\n",
      "|   MO|   16|\n",
      "|   MI|   14|\n",
      "|   TX|   13|\n",
      "+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Analysis of bank failures over time\n",
    "from pyspark.sql.functions import col, countDistinct, isnan, when, count, year, to_date, col, substring, concat, lit, avg\n",
    "from pyspark.sql.types import *\n",
    "print(\"\\n Bank failures by year: \")\n",
    "df.withColumn(\"Year\", year(\"Closing Date\")).groupBy(\"Year\").count().orderBy(\"Year\").show()\n",
    "\n",
    "print(\"\\nTop 10 states with most bank failures: \")\n",
    "df.groupBy(\"State\").count().orderBy(col(\"count\").desc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|Closing Date|\n",
      "+------------+\n",
      "|26-Apr-24   |\n",
      "|3-Nov-23    |\n",
      "|28-Jul-23   |\n",
      "|1-May-23    |\n",
      "|12-Mar-23   |\n",
      "|10-Mar-23   |\n",
      "|23-Oct-20   |\n",
      "|16-Oct-20   |\n",
      "|3-Apr-20    |\n",
      "|14-Feb-20   |\n",
      "+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Trying to figure out why the year's arent showing properly:\n",
    "df.select(\"Closing Date\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Bank Name: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Cert: integer (nullable = true)\n",
      " |-- Acquiring Institution: string (nullable = true)\n",
      " |-- Closing Date: date (nullable = true)\n",
      " |-- Fund: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('Closing Date', to_date(col('Closing Date'), 'd-MMM-yy'))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bank failures by year:\n",
      "+----+-----+\n",
      "|Year|count|\n",
      "+----+-----+\n",
      "|2000|    2|\n",
      "|2001|    4|\n",
      "|2002|   11|\n",
      "|2003|    3|\n",
      "|2004|    4|\n",
      "|2007|    3|\n",
      "|2008|   25|\n",
      "|2009|  140|\n",
      "|2010|  157|\n",
      "|2011|   92|\n",
      "|2012|   51|\n",
      "|2013|   24|\n",
      "|2014|   18|\n",
      "|2015|    8|\n",
      "|2016|    5|\n",
      "|2017|    8|\n",
      "|2019|    4|\n",
      "|2020|    4|\n",
      "|2023|    5|\n",
      "|2024|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#now that we casted the variable to a date object, we can extract the year\n",
    "df = df.withColumn('Year', year(col('Closing Date')))\n",
    "print(\"\\nBank failures by year:\")\n",
    "df.groupBy(\"Year\").count().orderBy(\"Year\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This shows the bank failures were relatively low 2000-2007 (minor bump in 2002), spike from 2008-2011 coinciding with the global financial crisis, and post 2012 decreased onwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+-------------+-----+-----+-----------------------------------+------------+-----+----+\n",
      "|Bank Name                            |City         |State|Cert |Acquiring Institution              |Closing Date|Fund |Year|\n",
      "+-------------------------------------+-------------+-----+-----+-----------------------------------+------------+-----+----+\n",
      "|Republic First Bank dba Republic Bank|Philadelphia |PA   |27332|Fulton Bank, National Association  |2024-04-26  |10546|2024|\n",
      "|Citizens Bank                        |Sac City     |IA   |8758 |Iowa Trust & Savings Bank          |2023-11-03  |10545|2023|\n",
      "|Heartland Tri-State Bank             |Elkhart      |KS   |25851|Dream First Bank, N.A.             |2023-07-28  |10544|2023|\n",
      "|First Republic Bank                  |San Francisco|CA   |59017|JPMorgan Chase Bank, N.A.          |2023-05-01  |10543|2023|\n",
      "|Signature Bank                       |New York     |NY   |57053|Flagstar Bank, N.A.                |2023-03-12  |10540|2023|\n",
      "|Silicon Valley Bank                  |Santa Clara  |CA   |24735|First�Citizens Bank & Trust Company|2023-03-10  |10539|2023|\n",
      "+-------------------------------------+-------------+-----+-----+-----------------------------------+------------+-----+----+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(6, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+-----+-----+---------------------+------------+-----+----+\n",
      "|          Bank Name|       City|State| Cert|Acquiring Institution|Closing Date| Fund|Year|\n",
      "+-------------------+-----------+-----+-----+---------------------+------------+-----+----+\n",
      "|Silicon Valley Bank|Santa Clara|   CA|24735| First�Citizens Ba...|  2023-03-10|10539|2023|\n",
      "+-------------------+-----------+-----+-----+---------------------+------------+-----+----+\n",
      "\n",
      "+-------------------+-----------+-----+-----+---------------------+------------+-----+----+\n",
      "|          Bank Name|       City|State| Cert|Acquiring Institution|Closing Date| Fund|Year|\n",
      "+-------------------+-----------+-----+-----+---------------------+------------+-----+----+\n",
      "|Silicon Valley Bank|Santa Clara|   CA|24735| First�Citizens Ba...|  2023-03-10|10539|2023|\n",
      "+-------------------+-----------+-----+-----+---------------------+------------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Manually pulling out the unclean row\n",
    "df.filter(col('Cert') == 24735).show()\n",
    "\n",
    "#defining a regex pattern to match non-ascii characters\n",
    "non_ascii = r'[^\\x00-\\x7F]'\n",
    "weird_rows = df.filter(col('Acquiring Institution').rlike(non_ascii)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+-----+-----+-----------------------------------+------------+-----+----+\n",
      "|Bank Name          |City       |State|Cert |Acquiring Institution              |Closing Date|Fund |Year|\n",
      "+-------------------+-----------+-----+-----+-----------------------------------+------------+-----+----+\n",
      "|Silicon Valley Bank|Santa Clara|CA   |24735|First-Citizens Bank & Trust Company|2023-03-10  |10539|2023|\n",
      "+-------------------+-----------+-----+-----+-----------------------------------+------------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Just to make sure all columns are clean:\n",
    "\n",
    "columns_to_clean = ['Bank Name', 'City', 'State', 'Cert', 'Acquiring Institution', 'Closing Date', 'Fund']\n",
    "for column_name in columns_to_clean:\n",
    "    df = df.withColumn(\n",
    "        column_name,\n",
    "        regexp_replace(col(column_name), non_ascii, '-')\n",
    "    )\n",
    "df.filter(col(\"Bank Name\") == 'Silicon Valley Bank').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+-----+-----+-----------------------------------+------------+-----+----+\n",
      "|Bank Name          |City       |State|Cert |Acquiring Institution              |Closing Date|Fund |Year|\n",
      "+-------------------+-----------+-----+-----+-----------------------------------+------------+-----+----+\n",
      "|Silicon Valley Bank|Santa Clara|CA   |24735|First-Citizens Bank & Trust Company|2023-03-10  |10539|2023|\n",
      "+-------------------+-----------+-----+-----+-----------------------------------+------------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#I noticed first citizens bank doesn't have the \"-\" like the others, so lets fix that\n",
    "df = df.withColumn(\n",
    "    'Acquiring Institution',\n",
    "    regexp_replace('Acquiring Institution', 'FirstCitizens', 'First-Citizens')\n",
    ")\n",
    "df.filter(col('Bank Name') == 'Silicon Valley Bank').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding specific data needs:**\n",
    "\n",
    "As of this block I have cleaned the FDIC Failed Bank List. So far I have discussed conducting time-series analysis on the various financial institutions being acquired, and seeing what time period was common and from the 303 acquiring institutions out of the 569 total entries, seeing what (if any) specific institution was more common than the others and figuring it out why. Now that I have the bank failure list, I am going to add on datasets that represent general economic indicators and specific financial trends. I am going to add on a GDP growth rate dataset along with some Cryptocurrency market capitalization data so we can compare the failures to different economic periods and crypto trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Bitcoin data from CoinGecko since 2013. I chose Bitcoin because it has always been the largest cryptocurrency and highly reflective of the market. Also as a student this data is not readily available, and the actual total market cap of all of crypto value is locked behind a paywall of $1000 a month (https://coinmarketcap.com/crypto-heatmap/). I also got the gdp data from https://www.bea.gov/data/gdp/gross-domestic-product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's check the overall schema for both datasets (GDP and crypto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------------------+---------------------------------------+\n",
      "|   _c0|GDP in billions of current dollars|GDP in billions of chained 2017 dollars|\n",
      "+------+----------------------------------+---------------------------------------+\n",
      "|2000Q1|                               4.2|                                    1.5|\n",
      "|2000Q2|                              10.2|                                    7.5|\n",
      "|2000Q3|                               2.8|                                    0.4|\n",
      "|2000Q4|                               4.6|                                    2.4|\n",
      "|2001Q1|                               1.3|                                   -1.3|\n",
      "+------+----------------------------------+---------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- GDP in billions of current dollars: double (nullable = true)\n",
      " |-- GDP in billions of chained 2017 dollars: double (nullable = true)\n",
      "\n",
      "+-------------------+------+-------------+------------+\n",
      "|         snapped_at| price|   market_cap|total_volume|\n",
      "+-------------------+------+-------------+------------+\n",
      "|2013-04-27 20:00:00| 135.3| 1.50051759E9|         0.0|\n",
      "|2013-04-28 20:00:00|141.96|1.575032004E9|         0.0|\n",
      "|2013-04-29 20:00:00| 135.3|1.501657493E9|         0.0|\n",
      "|2013-04-30 20:00:00| 117.0| 1.29895155E9|         0.0|\n",
      "|2013-05-01 20:00:00|103.43|1.148667722E9|         0.0|\n",
      "+-------------------+------+-------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- snapped_at: timestamp (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- market_cap: double (nullable = true)\n",
      " |-- total_volume: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, coalesce, avg\n",
    "gdp_df = spark.read.csv('gdp.csv', header=True, inferSchema=True)\n",
    "gdp_df.show(5)\n",
    "gdp_df.printSchema()\n",
    "\n",
    "btc_df = spark.read.csv('btc-usd-max.csv', header=True, inferSchema=True)\n",
    "btc_df.show(5)\n",
    "btc_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning & preprocessing of the GDP dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-------+------------------+\n",
      "|  Date|Year|Quarter|Quarter_Start_Date|\n",
      "+------+----+-------+------------------+\n",
      "|2000Q1|2000|      1|        2000-01-01|\n",
      "|2000Q2|2000|      2|        2000-04-01|\n",
      "|2000Q3|2000|      3|        2000-07-01|\n",
      "|2000Q4|2000|      4|        2000-10-01|\n",
      "|2001Q1|2001|      1|        2001-01-01|\n",
      "+------+----+-------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "----------------------------\n",
      "+------+-------------------+------------------------+----+-------+------------------+\n",
      "|  Date|GDP_Current_Dollars|GDP_Chained_2017_Dollars|Year|Quarter|Quarter_Start_Date|\n",
      "+------+-------------------+------------------------+----+-------+------------------+\n",
      "|2000Q1|                4.2|                     1.5|2000|      1|        2000-01-01|\n",
      "|2000Q2|               10.2|                     7.5|2000|      2|        2000-04-01|\n",
      "|2000Q3|                2.8|                     0.4|2000|      3|        2000-07-01|\n",
      "|2000Q4|                4.6|                     2.4|2000|      4|        2000-10-01|\n",
      "|2001Q1|                1.3|                    -1.3|2001|      1|        2001-01-01|\n",
      "|2001Q2|                5.0|                     2.5|2001|      2|        2001-04-01|\n",
      "|2001Q3|                0.0|                    -1.6|2001|      3|        2001-07-01|\n",
      "|2001Q4|                2.4|                     1.1|2001|      4|        2001-10-01|\n",
      "|2002Q1|                4.7|                     3.4|2002|      1|        2002-01-01|\n",
      "|2002Q2|                3.9|                     2.5|2002|      2|        2002-04-01|\n",
      "|2002Q3|                3.6|                     1.6|2002|      3|        2002-07-01|\n",
      "|2002Q4|                2.8|                     0.5|2002|      4|        2002-10-01|\n",
      "|2003Q1|                4.1|                     2.1|2003|      1|        2003-01-01|\n",
      "|2003Q2|                5.1|                     3.6|2003|      2|        2003-04-01|\n",
      "|2003Q3|                9.3|                     6.8|2003|      3|        2003-07-01|\n",
      "+------+-------------------+------------------------+----+-------+------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#renaming this column to something referencable\n",
    "gdp_df = gdp_df.withColumnRenamed('GDP in billions of current dollars', 'GDP_Current_Dollars') \\\n",
    "               .withColumnRenamed('GDP in billions of chained 2017 dollars', 'GDP_Chained_2017_Dollars') \\\n",
    "               .withColumnRenamed('_c0', 'Date')\n",
    "\n",
    "#creating a date-time object to represent the first day of the quarter\n",
    "\n",
    "gdp_df = gdp_df.withColumn('Year', substring('Date', 1, 4).cast('int'))\n",
    "gdp_df = gdp_df.withColumn('Quarter', substring('Date', 6, 1).cast('int'))\n",
    "#line below converts the string to a date type, concatenates the year with a hyphen, then gets the quarter by doing * 3 - 2 so i.e. Q4 is 4 * 3 - 2 so \n",
    "#its october or 10 which makes sense, then the day 01, and then the format of yyyy-M-d \n",
    "gdp_df = gdp_df.withColumn('Quarter_Start_Date', to_date(concat(col('Year'), lit('-'), (col('Quarter') * 3 - 2), lit('-01')), 'yyyy-M-d'))\n",
    "\n",
    "gdp_df.select('Date', 'Year', 'Quarter', 'Quarter_Start_Date').show(5)\n",
    "print(\"----------------------------\")\n",
    "gdp_df.show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get the annual GDP growth rate by averaging the quarterly growth rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+\n",
      "|Year|  Annual_GDP_Growth|\n",
      "+----+-------------------+\n",
      "|2000|  5.449999999999999|\n",
      "|2001|              2.175|\n",
      "|2002|               3.75|\n",
      "|2003|               6.45|\n",
      "|2004|                6.4|\n",
      "|2005|              6.375|\n",
      "|2006|              5.375|\n",
      "|2007|                4.8|\n",
      "|2008|-0.6249999999999998|\n",
      "|2009| 0.3500000000000003|\n",
      "|2010|                4.5|\n",
      "|2011| 3.4999999999999996|\n",
      "|2012| 3.6500000000000004|\n",
      "|2013|                4.7|\n",
      "|2014|              4.225|\n",
      "|2015|              2.925|\n",
      "|2016|               3.55|\n",
      "|2017|              4.975|\n",
      "|2018|                4.4|\n",
      "|2019|               4.85|\n",
      "+----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "annual_gdp_df = gdp_df.groupBy('Year').agg(avg('GDP_Current_Dollars').alias('Annual_GDP_Growth'))\n",
    "annual_gdp_df = annual_gdp_df.orderBy('Year')\n",
    "annual_gdp_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning and preprocessing of the bitcoin dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- price: double (nullable = true)\n",
      " |-- market_cap: double (nullable = true)\n",
      " |-- total_volume: double (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "btc_df = btc_df.withColumn('Date', to_date(col('snapped_at'), 'yyyy-MM-dd HH:mm:ss z'))\n",
    "btc_df = btc_df.drop('snapped_at')\n",
    "btc_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+------------+----------+----+-----+\n",
      "| price|   market_cap|total_volume|      Date|Year|Month|\n",
      "+------+-------------+------------+----------+----+-----+\n",
      "| 135.3| 1.50051759E9|         0.0|2013-04-27|2013|    4|\n",
      "|141.96|1.575032004E9|         0.0|2013-04-28|2013|    4|\n",
      "| 135.3|1.501657493E9|         0.0|2013-04-29|2013|    4|\n",
      "| 117.0| 1.29895155E9|         0.0|2013-04-30|2013|    4|\n",
      "|103.43|1.148667722E9|         0.0|2013-05-01|2013|    5|\n",
      "+------+-------------+------------+----------+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import month\n",
    "btc_df = btc_df.withColumn('Year', year(col('Date')))\n",
    "btc_df = btc_df.withColumn('Month', month(col('Date')))\n",
    "btc_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------------+----+----+-----+\n",
      "|price|market_cap|total_volume|Date|Year|Month|\n",
      "+-----+----------+------------+----+----+-----+\n",
      "|    0|         1|           0|   0|   0|    0|\n",
      "+-----+----------+------------+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking for missing values in market cap\n",
    "btc_df.select([count(when(col(c).isNull(), c)).alias(c) for c in btc_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------------+----+----+-----+\n",
      "|price|market_cap|total_volume|Date|Year|Month|\n",
      "+-----+----------+------------+----+----+-----+\n",
      "|    0|         0|           0|   0|   0|    0|\n",
      "+-----+----------+------------+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "btc_df = btc_df.dropna(subset=['market_cap'])\n",
    "btc_df.select([count(when(col(c).isNull(), c)).alias(c) for c in btc_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|Year|   Annual_Market_Cap|\n",
      "+----+--------------------+\n",
      "|2013| 3.077474372939516E9|\n",
      "|2014| 6.756528320092427E9|\n",
      "|2015| 3.925042751283574E9|\n",
      "|2016| 8.937041445128815E9|\n",
      "|2017|6.746620332320473...|\n",
      "|2018|1.291900764703961E11|\n",
      "|2019|1.315301910601856...|\n",
      "|2020|2.048368551120182E11|\n",
      "|2021|8.908025690535553E11|\n",
      "|2022|5.375604359614069E11|\n",
      "|2023| 5.60908038513991E11|\n",
      "|2024|1.184769581052524E12|\n",
      "+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "annual_btc_df = btc_df.groupBy('Year').agg(avg('market_cap').alias('Annual_Market_Cap'))\n",
    "annual_btc_df = annual_btc_df.orderBy('Year')\n",
    "annual_btc_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# btc_df = btc_df.filter(col('market_cap') > 0) #removing non 0 entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging datasets with bank failure data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+\n",
      "|Year|Num_Bank_Failures|\n",
      "+----+-----------------+\n",
      "|2000|                2|\n",
      "|2001|                4|\n",
      "|2002|               11|\n",
      "|2003|                3|\n",
      "|2004|                4|\n",
      "|2007|                3|\n",
      "|2008|               25|\n",
      "|2009|              140|\n",
      "|2010|              157|\n",
      "|2011|               92|\n",
      "|2012|               51|\n",
      "|2013|               24|\n",
      "|2014|               18|\n",
      "|2015|                8|\n",
      "|2016|                5|\n",
      "|2017|                8|\n",
      "|2019|                4|\n",
      "|2020|                4|\n",
      "|2023|                5|\n",
      "|2024|                1|\n",
      "+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bank_failures_per_year = df.groupBy('Year').count().orderBy('Year').withColumnRenamed('count', 'Num_Bank_Failures')\n",
    "bank_failures_per_year.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+-------------------+\n",
      "|Year|Num_Bank_Failures|  Annual_GDP_Growth|\n",
      "+----+-----------------+-------------------+\n",
      "|2003|                3|               6.45|\n",
      "|2007|                3|                4.8|\n",
      "|2015|                8|              2.925|\n",
      "|2023|                5|               5.85|\n",
      "|2013|               24|                4.7|\n",
      "|2014|               18|              4.225|\n",
      "|2019|                4|               4.85|\n",
      "|2004|                4|                6.4|\n",
      "|2020|                4|  3.624999999999999|\n",
      "|2012|               51| 3.6500000000000004|\n",
      "|2009|              140| 0.3500000000000003|\n",
      "|2016|                5|               3.55|\n",
      "|2001|                4|              2.175|\n",
      "|2024|                1|               5.15|\n",
      "|2000|                2|  5.449999999999999|\n",
      "|2010|              157|                4.5|\n",
      "|2011|               92| 3.4999999999999996|\n",
      "|2008|               25|-0.6249999999999998|\n",
      "|2017|                8|              4.975|\n",
      "|2002|               11|               3.75|\n",
      "+----+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bank_gdp_df = bank_failures_per_year.join(annual_gdp_df, on='Year', how='left')\n",
    "bank_gdp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+-------------------+--------------------+\n",
      "|Year|Num_Bank_Failures|  Annual_GDP_Growth|   Annual_Market_Cap|\n",
      "+----+-----------------+-------------------+--------------------+\n",
      "|2003|                3|               6.45|                 0.0|\n",
      "|2007|                3|                4.8|                 0.0|\n",
      "|2015|                8|              2.925| 3.925042751283574E9|\n",
      "|2023|                5|               5.85| 5.60908038513991E11|\n",
      "|2013|               24|                4.7| 3.077474372939516E9|\n",
      "|2014|               18|              4.225| 6.756528320092427E9|\n",
      "|2019|                4|               4.85|1.315301910601856...|\n",
      "|2004|                4|                6.4|                 0.0|\n",
      "|2020|                4|  3.624999999999999|2.048368551120182E11|\n",
      "|2012|               51| 3.6500000000000004|                 0.0|\n",
      "|2009|              140| 0.3500000000000003|                 0.0|\n",
      "|2016|                5|               3.55| 8.937041445128815E9|\n",
      "|2001|                4|              2.175|                 0.0|\n",
      "|2024|                1|               5.15|1.184769581052524E12|\n",
      "|2000|                2|  5.449999999999999|                 0.0|\n",
      "|2010|              157|                4.5|                 0.0|\n",
      "|2011|               92| 3.4999999999999996|                 0.0|\n",
      "|2008|               25|-0.6249999999999998|                 0.0|\n",
      "|2017|                8|              4.975|6.746620332320473...|\n",
      "|2002|               11|               3.75|                 0.0|\n",
      "+----+-----------------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bank_gdp_btc_df = bank_gdp_df.join(annual_btc_df, on='Year', how='left')\n",
    "bank_gdp_btc_df = bank_gdp_btc_df.fillna({'Annual_Market_Cap': 0})\n",
    "bank_gdp_btc_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Looking for potential issues in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing years in GDP data: set()\n"
     ]
    }
   ],
   "source": [
    "#checking to make sure there are no missing years in the GDP data, and verifying the output is numeric\n",
    "expected_years = range(bank_failures_per_year.agg({\"Year\": \"min\"}).collect()[0][0],\n",
    "                       bank_failures_per_year.agg({\"Year\": \"max\"}).collect()[0][0] + 1)\n",
    "missing_years = set(expected_years) - set([row['Year'] for row in annual_gdp_df.collect()])\n",
    "print(f\"Missing years in GDP data: {missing_years}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In terms of potential issues in the Bitcoin data, the data starts from 2013, all years prior to 2013 the annual market cap for BTC is set to zero. We already checked the original FDIC failed bank list for any missing or null values. \n",
    "\n",
    "##### For the FDIC list: \n",
    "* We cleared up the random \"?\" symbols, \n",
    "* found the number of unique entries matched the number of total entries through verifying via the cert, \n",
    "* checked for non-numeric values in cert & fund columns and found nothing wrong, \n",
    "* then casted the 'Closing Date' variable to be a date so we could sort different events by dates, \n",
    "* finally, to ensure each and every entry of every row/column was valid, we sorted through and identified any entry/cell that had a non-ascii character and fixed it via locating where it was (because we had already done so much prior pre-processing, only a couple data points had this issue), and adjusting those non-ascii entries to be proper (i.e. citizens-bank)\n",
    "* besides cleaning the data, there only true variables you could quantify to look at for outliers are cert & fund, which would be incorrect to calculate a z-score for because those variables are identifiers for the acquisition being seen, it doesn't necessarily mean the entry is invalid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying potential issues in GDP data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+------------------------+----+-------+------------------+\n",
      "|  Date|GDP_Current_Dollars|GDP_Chained_2017_Dollars|Year|Quarter|Quarter_Start_Date|\n",
      "+------+-------------------+------------------------+----+-------+------------------+\n",
      "|2000Q1|                4.2|                     1.5|2000|      1|        2000-01-01|\n",
      "|2000Q2|               10.2|                     7.5|2000|      2|        2000-04-01|\n",
      "|2000Q3|                2.8|                     0.4|2000|      3|        2000-07-01|\n",
      "|2000Q4|                4.6|                     2.4|2000|      4|        2000-10-01|\n",
      "|2001Q1|                1.3|                    -1.3|2001|      1|        2001-01-01|\n",
      "+------+-------------------+------------------------+----+-------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- GDP_Current_Dollars: double (nullable = true)\n",
      " |-- GDP_Chained_2017_Dollars: double (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Quarter: integer (nullable = true)\n",
      " |-- Quarter_Start_Date: date (nullable = true)\n",
      "\n",
      "Total rows: 98\n",
      "Distinct rows: 98\n",
      "Duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "gdp_df.show(5)\n",
    "gdp_df.printSchema()\n",
    "\n",
    "# total rows\n",
    "total_rows = gdp_df.count()\n",
    "# distinct rows\n",
    "distinct_rows = gdp_df.dropDuplicates().count()\n",
    "# duplicates\n",
    "duplicate_rows = total_rows - distinct_rows\n",
    "print(f\"Total rows: {total_rows}\")\n",
    "print(f\"Distinct rows: {distinct_rows}\")\n",
    "print(f\"Duplicate rows: {duplicate_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+------------------------+----+-------+------------------+\n",
      "|Date|GDP_Current_Dollars|GDP_Chained_2017_Dollars|Year|Quarter|Quarter_Start_Date|\n",
      "+----+-------------------+------------------------+----+-------+------------------+\n",
      "|   0|                  0|                       0|   0|      0|                 0|\n",
      "+----+-------------------+------------------------+----+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, count\n",
    "\n",
    "#check for nulls in each column\n",
    "null_counts = gdp_df.select([count(when(col(c).isNull(), c)).alias(c) for c in gdp_df.columns])\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- GDP_Current_Dollars: double (nullable = true)\n",
      " |-- GDP_Chained_2017_Dollars: double (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Quarter: integer (nullable = true)\n",
      " |-- Quarter_Start_Date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking for type inconsistencies\n",
    "gdp_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already converted the date column to a proper datetype:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# +------+----+-------+------------------+\n",
    "# |  Date|Year|Quarter|Quarter_Start_Date|\n",
    "# +------+----+-------+------------------+\n",
    "# |2000Q1|2000|      1|        2000-01-01|\n",
    "# |2000Q2|2000|      2|        2000-04-01|\n",
    "# |2000Q3|2000|      3|        2000-07-01|\n",
    "# |2000Q4|2000|      4|        2000-10-01|\n",
    "# |2001Q1|2001|      1|        2001-01-01|\n",
    "# +------+----+-------+------------------+\n",
    "# only showing top 5 rows\n",
    "\n",
    "# ----------------------------\n",
    "# +------+-------------------+------------------------+----+-------+------------------+\n",
    "# |  Date|GDP_Current_Dollars|GDP_Chained_2017_Dollars|Year|Quarter|Quarter_Start_Date|\n",
    "# +------+-------------------+------------------------+----+-------+------------------+\n",
    "# |2000Q1|                4.2|                     1.5|2000|      1|        2000-01-01|\n",
    "# |2000Q2|               10.2|                     7.5|2000|      2|        2000-04-01|\n",
    "# |2000Q3|                2.8|                     0.4|2000|      3|        2000-07-01|\n",
    "# |2000Q4|                4.6|                     2.4|2000|      4|        2000-10-01|\n",
    "# |2001Q1|                1.3|                    -1.3|2001|      1|        2001-01-01|\n",
    "# |2001Q2|                5.0|                     2.5|2001|      2|        2001-04-01|\n",
    "# |2001Q3|                0.0|                    -1.6|2001|      3|        2001-07-01|\n",
    "# |2001Q4|                2.4|                     1.1|2001|      4|        2001-10-01|\n",
    "# |2002Q1|                4.7|                     3.4|2002|      1|        2002-01-01|\n",
    "# |2002Q2|                3.9|                     2.5|2002|      2|        2002-04-01|\n",
    "# |2002Q3|                3.6|                     1.6|2002|      3|        2002-07-01|\n",
    "# |2002Q4|                2.8|                     0.5|2002|      4|        2002-10-01|\n",
    "# |2003Q1|                4.1|                     2.1|2003|      1|        2003-01-01|\n",
    "# |2003Q2|                5.1|                     3.6|2003|      2|        2003-04-01|\n",
    "# |2003Q3|                9.3|                     6.8|2003|      3|        2003-07-01|\n",
    "# +------+-------------------+------------------------+----+-------+------------------+\n",
    "# only showing top 15 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+------------------+\n",
      "|  Date|GDP_Current_Dollars|       GDP_Z_Score|\n",
      "+------+-------------------+------------------+\n",
      "|2020Q2|              -29.1|-5.711285067478292|\n",
      "|2020Q3|               40.0| 5.980628297481198|\n",
      "+------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking for outliers in GDP growth rate\n",
    "from pyspark.sql.functions import mean as _mean, stddev as _stddev, col\n",
    "\n",
    "# mean and standard deviation\n",
    "mean_val = gdp_df.select(_mean(col('GDP_Current_Dollars'))).collect()[0][0]\n",
    "stddev_val = gdp_df.select(_stddev(col('GDP_Current_Dollars'))).collect()[0][0]\n",
    "\n",
    "# adding Z-score column\n",
    "gdp_df = gdp_df.withColumn('GDP_Z_Score', (col('GDP_Current_Dollars') - mean_val) / stddev_val)\n",
    "\n",
    "# identifying outliers (Z-score > 3 or < -3)\n",
    "gdp_outliers = gdp_df.filter((col('GDP_Z_Score') > 3) | (col('GDP_Z_Score') < -3))\n",
    "gdp_outliers.select('Date', 'GDP_Current_Dollars', 'GDP_Z_Score').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Although these are outliers, this makes sense and the entries should stay because this was clearly during the COVID-19 pandemic hence the intense volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the bitcoin dataset for potential issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- price: double (nullable = true)\n",
      " |-- market_cap: double (nullable = true)\n",
      " |-- total_volume: double (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      "\n",
      "Total rows: 4180\n",
      "Distinct rows: 4180\n",
      "Duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "btc_df.printSchema()\n",
    "# total rows\n",
    "total_rows = btc_df.count()\n",
    "# distinct rows\n",
    "distinct_rows = btc_df.dropDuplicates().count()\n",
    "# calculating duplicates\n",
    "duplicate_rows = total_rows - distinct_rows\n",
    "print(f\"Total rows: {total_rows}\")\n",
    "print(f\"Distinct rows: {distinct_rows}\")\n",
    "print(f\"Duplicate rows: {duplicate_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------------+----+----+-----+\n",
      "|price|market_cap|total_volume|Date|Year|Month|\n",
      "+-----+----------+------------+----+----+-----+\n",
      "|    0|         0|           0|   0|   0|    0|\n",
      "+-----+----------+------------+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# above shows no type inconsistencies, date is date type\n",
    "# checking for nulls in each column\n",
    "null_counts = btc_df.select([count(when(col(c).isNull(), c)).alias(c) for c in btc_df.columns])\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So because bitcoin is so volatile and has had exponential growth, so we can do log transformation before calculating z-scores to normalize the data a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------------------+\n",
      "|Date|market_cap|Market_Cap_Z_Score|\n",
      "+----+----------+------------------+\n",
      "+----+----------+------------------+\n",
      "\n",
      "---------------------------------seeing z score without logging-------------------------\n",
      "+----+----------+-------------------------+\n",
      "|Date|market_cap|Market_Cap_Z_Score_no_log|\n",
      "+----+----------+-------------------------+\n",
      "+----+----------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import log\n",
    "\n",
    "# applying log to market cap\n",
    "btc_df = btc_df.withColumn('log_market_cap', log(col('market_cap')))\n",
    "\n",
    "# getting mean and stdev\n",
    "mean_log_mc = btc_df.select(_mean(col('log_market_cap'))).collect()[0][0]\n",
    "stddev_log_mc = btc_df.select(_stddev(col('log_market_cap'))).collect()[0][0]\n",
    "\n",
    "# adding Z-score column\n",
    "btc_df = btc_df.withColumn('Market_Cap_Z_Score', (col('log_market_cap') - mean_log_mc) / stddev_log_mc)\n",
    "\n",
    "# seeing any outliers\n",
    "btc_outliers = btc_df.filter((col('Market_Cap_Z_Score') > 3) | (col('Market_Cap_Z_Score') < -3))\n",
    "btc_outliers.select('Date', 'market_cap', 'Market_Cap_Z_Score').show()\n",
    "\n",
    "print(\"---------------------------------seeing z score without logging-------------------------\")\n",
    "mean_log_mc = btc_df.select(_mean(col('market_cap'))).collect()[0][0]\n",
    "stddev_log_mc = btc_df.select(_stddev(col('market_cap'))).collect()[0][0]\n",
    "btc_df = btc_df.withColumn('Market_Cap_Z_Score_no_log', (col('market_cap') - mean_log_mc) / stddev_log_mc)\n",
    "btc_outliers = btc_df.filter((col('Market_Cap_Z_Score_no_log') > 3) | (col('Market_Cap_Z_Score_no_log') < -3))\n",
    "btc_outliers.select('Date', 'market_cap', 'Market_Cap_Z_Score_no_log').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of GDP & bitcoin data:\n",
    "* Duplicates: No duplicate rows were found in the datasets\n",
    "* Missing Values: No missing or null values, for btc no missing values in the 'market_cap' column were found except for initial days where 'total_volume' is zero, which we are not using in our analysis \n",
    "* Type Consistency: all columns are correctly typed\n",
    "* Outliers: the outliers in the GDP set were identified using Z-scores which correspond to significant economic events like 2008 crisis so I kept those data points as they are crucial for our analysis later.\n",
    "* Date Parsing: All dates have been successfully parsed into proper date formats, covering all quarters from 2000 to the latest available data.\n",
    "* Conclusion: both datasets, after heavy pre-processing and cleaning are now ready for analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Any reorganization needed? No. All variables have valid variable names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed plan (ALREADY DONE IN PREVIOUS STEPS)\n",
    "* GDP market cap: |Date|GDP_Current_Dollars|GDP_Chained_2017_Dollars|Year|Quarter|Quarter_Start_Date|\n",
    "* BTC: |price|market_cap|total_volume|Date|Year|Month|\n",
    "* FDIC: |summary|Bank Name|City|State|Cert|Acquiring Institution|Closing Date|Fund|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FDIC Dataset Cleaning Plan:\n",
    "\n",
    "Issue: Non-ASCII characters in column names and entries\n",
    "* Action: use regex to remove/replace non-ASCII characters\n",
    "* Technique: use regexp_replace() function to clean data\n",
    "* Why: makes sure data is consistent and prevents encoding issues\n",
    "\n",
    "Issue: bad parsing of 'Closing Date' leading to null values\n",
    "* Action: fix the date parsing format and adjust years \n",
    "* Technique: Use to_date() with the correct format and adjust two-digit year\n",
    "* Why: date parsing is important for time-based analysis\n",
    "\n",
    "Issue: duplicate entries\n",
    "* Action: Check for and remove duplicate rows\n",
    "* Technique: Use dropDuplicates() function\n",
    "* Why: gets rid of redundant data that could skew analysis\n",
    "\n",
    "GDP Dataset Cleaning Plan:\n",
    "\n",
    "Issue: missing/null values\n",
    "* Action: check if missing values exist and decide whether to include/exclude them\n",
    "* Technique: Use dropna() or fillna() as appropriate\n",
    "* Why: makes sure data is complete and with integrity\n",
    "Issue: outliers in GDP growth rates\n",
    "* Action: find and assess outliers to determine if they reflect actual economic events\n",
    "* Technique: get Z-scores and use domain knowledge for interpretation (i.e. seing high gdp outlier with 08)\n",
    "* Why: ensures validity of the dataset by keeping significant data points\n",
    "\n",
    "Bitcoin Dataset Cleaning Plan:\n",
    "\n",
    "Issue: exponential growth causing skewness in market cap data\n",
    "* Action: apply log transformation to normalize data\n",
    "* Technique: Use log() function on 'Market_Cap_USD'\n",
    "* Why: normalizes data for better analysis\n",
    "Issue: missing dates or zero market cap values.\n",
    "* Action: check and handle missing or zero values appropriately\n",
    "* Technique: Use explanatory notes (i.e. dates before 2013 are 0 because we dont have them/bitcoin didnt exist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o695.csv.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:78)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:567)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:831)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-546ced7b829e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgdp_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cleaned_gdp.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'overwrite'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mbtc_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cleaned_btc.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'overwrite'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cleaned_bank.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'overwrite'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1862\u001b[0m             \u001b[0mlineSep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlineSep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1863\u001b[0m         )\n\u001b[1;32m-> 1864\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m     def orc(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o695.csv.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:78)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:567)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:831)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n"
     ]
    }
   ],
   "source": [
    "gdp_df.write.csv('cleaned_gdp.csv', header=True, mode='overwrite')\n",
    "btc_df.write.csv('cleaned_btc.csv', header=True, mode='overwrite')\n",
    "df.write.csv('cleaned_bank.csv', header=True, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
