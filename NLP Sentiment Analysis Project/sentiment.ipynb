{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              publishedAt                                              title  \\\n",
      "0    2024-06-02T23:58:41Z         The Fastest Declining Large City In The US   \n",
      "1    2024-06-02T23:40:20Z  Mad Paws Holdings Limited (ASX:MPA): Are Analy...   \n",
      "2    2024-06-02T23:34:07Z  ARE : Partial Correction to “Notice of the 15t...   \n",
      "3    2024-06-02T23:24:33Z  Saudi Arabia’s oil giant sees massive stock of...   \n",
      "4    2024-06-02T23:13:05Z  Ships Diverted From Red Sea Send Ripple Effect...   \n",
      "..                    ...                                                ...   \n",
      "395  2024-06-23T17:26:45Z  PepsiCo, Inc. (NASDAQ:PEP) Shares Acquired by ...   \n",
      "396  2024-06-23T17:18:43Z  Triangle Securities Wealth Management Sells 48...   \n",
      "397  2024-06-23T17:18:42Z  Visa Inc. (NYSE:V) Holdings Lowered by Liberty...   \n",
      "398  2024-06-23T17:18:42Z  Maryland Capital Advisors Inc. Increases Posit...   \n",
      "399  2024-06-23T17:18:42Z  Rockland Trust Co. Has $36.26 Million Position...   \n",
      "\n",
      "                                           description  \n",
      "0    In this piece, we will take a look at the fast...  \n",
      "1    We feel now is a pretty good time to analyse M...  \n",
      "2    (marketscreener.com) \\n \\n TRANSLATION FOR REF...  \n",
      "3    Saudi Aramco’s $12 billion share sale sold out...  \n",
      "4    By Brendan Murray Jun 2, 2024 (Bloomberg) –Nev...  \n",
      "..                                                 ...  \n",
      "395  MJP Associates Inc. ADV boosted its holdings i...  \n",
      "396  Triangle Securities Wealth Management reduced ...  \n",
      "397  Liberty Capital Management Inc. trimmed its ho...  \n",
      "398  Maryland Capital Advisors Inc. raised its posi...  \n",
      "399  Rockland Trust Co. lowered its stake in shares...  \n",
      "\n",
      "[400 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from datetime import datetime, timedelta\n",
    "import yfinance as yf\n",
    "\n",
    "#Phase 1: get news data via newsapi.org\n",
    "\n",
    "# Set your API key\n",
    "api_key = '71015ea355524adaa5463cf1651763b6'\n",
    "\n",
    "# Define the date range\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=30) # free tier caps us off at 30 days, so maybe we can do a day by day analysis?\n",
    "\n",
    "articles_list = []\n",
    "\n",
    "for i in range(4):\n",
    "    chunk_start_date = start_date + timedelta(days=i*7)\n",
    "    chunk_end_date = chunk_start_date + timedelta(days=7)\n",
    "    \n",
    "    url = (f'https://newsapi.org/v2/everything?q=stock market&from={chunk_start_date.strftime(\"%Y-%m-%d\")}'\n",
    "           f'&to={chunk_end_date.strftime(\"%Y-%m-%d\")}&language=en&sortBy=publishedAt&apiKey={api_key}')\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        articles = response.json().get('articles', [])\n",
    "        articles_list.extend(articles)\n",
    "    else:\n",
    "        print(f\"Error fetching data: {response.status_code}\")\n",
    "\n",
    "# Store the articles in a pandas df\n",
    "df = pd.DataFrame(articles_list)\n",
    "print(df[['publishedAt', 'title', 'description']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2024-06-02\n",
      "1    2024-06-02\n",
      "2    2024-06-02\n",
      "3    2024-06-02\n",
      "4    2024-06-02\n",
      "5    2024-06-02\n",
      "6    2024-06-02\n",
      "7    2024-06-02\n",
      "8    2024-06-02\n",
      "9    2024-06-02\n",
      "Name: publishedAt, dtype: object\n",
      "          day  sentiment\n",
      "0  1970-01-01   0.000000\n",
      "1  2024-06-02   0.277242\n",
      "2  2024-06-09   0.244690\n",
      "3  2024-06-16   0.378506\n",
      "4  2024-06-23   0.379557\n",
      "        week  sentiment\n",
      "0 1969-12-29   0.000000\n",
      "1 2024-05-27   0.277242\n",
      "2 2024-06-03   0.244690\n",
      "3 2024-06-10   0.378506\n",
      "4 2024-06-17   0.379557\n",
      "Date range: 1970-01-01 to 2024-06-23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Rohan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Phase 2: Pre-processing & Sentiment Analysis\n",
    "nltk.download('vader_lexicon')\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to get the sentiment score\n",
    "def get_sentiment_score(text):\n",
    "    return sid.polarity_scores(text)['compound'] if text else 0\n",
    "\n",
    "df['sentiment'] = df['description'].apply(get_sentiment_score)\n",
    "\n",
    "# Convert 'publishedAt' to datetime and ensure dates are parsed correctly\n",
    "df['publishedAt'] = pd.to_datetime(df['publishedAt']).dt.date\n",
    "\n",
    "# Verify the parsing\n",
    "print(df['publishedAt'].head(10))\n",
    "\n",
    "# Group by day and calculate average sentiment\n",
    "df['day'] = df['publishedAt']\n",
    "daily_sentiment = df.groupby('day')['sentiment'].mean().reset_index()\n",
    "\n",
    "# Group by week and calculate average sentiment\n",
    "df['week'] = pd.to_datetime(df['day']).dt.to_period('W').apply(lambda r: r.start_time)\n",
    "weekly_sentiment = df.groupby('week')['sentiment'].mean().reset_index()\n",
    "\n",
    "# Display the daily and weekly sentiment\n",
    "print(daily_sentiment.head(10))\n",
    "print(weekly_sentiment.head(10))\n",
    "\n",
    "# Display the range of dates\n",
    "print(f\"Date range: {df['day'].min()} to {df['day'].max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Data with Daily Returns:\n",
      "           Date  daily_return\n",
      "0   2024-05-28           NaN\n",
      "1   2024-05-29     -0.007002\n",
      "2   2024-05-30     -0.006634\n",
      "3   2024-05-31      0.009108\n",
      "4   2024-06-03      0.000815\n",
      "5   2024-06-04      0.001118\n",
      "6   2024-06-05      0.011885\n",
      "7   2024-06-06     -0.000019\n",
      "8   2024-06-07     -0.001216\n",
      "9   2024-06-10      0.003090\n",
      "10  2024-06-11      0.002408\n",
      "11  2024-06-12      0.008213\n",
      "12  2024-06-13      0.002013\n",
      "13  2024-06-14      0.000608\n",
      "14  2024-06-17      0.007959\n",
      "15  2024-06-18      0.002541\n",
      "16  2024-06-20     -0.002717\n",
      "17  2024-06-21     -0.001341\n",
      "18  2024-06-24     -0.003251\n",
      "Stock Data with Weekly Returns:\n",
      "         week  weekly_return\n",
      "0 2024-05-27      -0.004605\n",
      "1 2024-06-03       0.011766\n",
      "2 2024-06-10       0.013292\n",
      "3 2024-06-17      -0.001523\n",
      "4 2024-06-24       0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#fetching stock market data for daily and weekly trends\n",
    "\n",
    "# Define the stock ticker and date range\n",
    "ticker = 'SPY'  # S&P 500 ETF as an example\n",
    "end_date = datetime.now().date()\n",
    "start_date = end_date - timedelta(days=30)\n",
    "stock_data = yf.download(ticker, start=start_date, end=end_date, interval='1d')\n",
    "\n",
    "# Calculate daily returns\n",
    "stock_data['daily_return'] = stock_data['Adj Close'].pct_change()\n",
    "\n",
    "# Reset the index to join with sentiment data\n",
    "stock_data.reset_index(inplace=True)\n",
    "\n",
    "# Convert Date to date\n",
    "stock_data['Date'] = stock_data['Date'].dt.date\n",
    "\n",
    "# Display the stock data\n",
    "print(\"Stock Data with Daily Returns:\\n\", stock_data[['Date', 'daily_return']])\n",
    "\n",
    "# Calculate weekly returns\n",
    "stock_data['week'] = pd.to_datetime(stock_data['Date']).dt.to_period('W').apply(lambda r: r.start_time)\n",
    "weekly_stock_data = stock_data.groupby('week').apply(lambda x: (x['Adj Close'].iloc[-1] - x['Adj Close'].iloc[0]) / x['Adj Close'].iloc[0]).reset_index()\n",
    "weekly_stock_data.columns = ['week', 'weekly_return']\n",
    "\n",
    "# Display the weekly stock data\n",
    "print(\"Stock Data with Weekly Returns:\\n\", weekly_stock_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weekly_stock_data.head(10))\n",
    "print(daily_sentiment.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Daily Sentiment Dates:\n",
      " [datetime.date(2024, 6, 2) datetime.date(2024, 6, 9)\n",
      " datetime.date(2024, 6, 16) datetime.date(2024, 6, 23)]\n",
      "Stock Data Dates:\n",
      " [datetime.date(2024, 5, 28) datetime.date(2024, 5, 29)\n",
      " datetime.date(2024, 5, 30) datetime.date(2024, 5, 31)\n",
      " datetime.date(2024, 6, 3) datetime.date(2024, 6, 4)\n",
      " datetime.date(2024, 6, 5) datetime.date(2024, 6, 6)\n",
      " datetime.date(2024, 6, 7) datetime.date(2024, 6, 10)\n",
      " datetime.date(2024, 6, 11) datetime.date(2024, 6, 12)\n",
      " datetime.date(2024, 6, 13) datetime.date(2024, 6, 14)\n",
      " datetime.date(2024, 6, 17) datetime.date(2024, 6, 18)\n",
      " datetime.date(2024, 6, 20) datetime.date(2024, 6, 21)\n",
      " datetime.date(2024, 6, 24)]\n",
      "Daily Combined Data:\n",
      " Empty DataFrame\n",
      "Columns: [day, sentiment, Date, daily_return]\n",
      "Index: []\n",
      "Daily Combined Data After Dropping NaNs:\n",
      " Empty DataFrame\n",
      "Columns: [day, sentiment, daily_return]\n",
      "Index: []\n",
      "Not enough data points to calculate daily correlation.\n"
     ]
    }
   ],
   "source": [
    "# Verify the date ranges in daily_sentiment again after filtering\n",
    "print(\"Filtered Daily Sentiment Dates:\\n\", daily_sentiment['day'].unique())\n",
    "\n",
    "# Verify the date ranges in stock_data\n",
    "print(\"Stock Data Dates:\\n\", stock_data['Date'].unique())\n",
    "\n",
    "# Merging daily sentiment and stock data after filtering invalid dates\n",
    "daily_combined_data = pd.merge(daily_sentiment, stock_data[['Date', 'daily_return']], left_on='day', right_on='Date', how='inner')\n",
    "\n",
    "# Display the daily combined data\n",
    "print(\"Daily Combined Data:\\n\", daily_combined_data.head(10))\n",
    "\n",
    "# Handle missing data\n",
    "daily_combined_data.dropna(subset=[# Verify the date ranges in daily_sentiment again after filtering\n",
    "print(\"Filtered Daily Sentiment Dates:\\n\", daily_sentiment['day'].unique())\n",
    "\n",
    "# Verify the date ranges in stock_data\n",
    "print(\"Stock Data Dates:\\n\", stock_data['Date'].unique())\n",
    "\n",
    "# Merging daily sentiment and stock data after filtering invalid dates\n",
    "daily_combined_data = pd.merge(daily_sentiment, stock_data[['Date', 'daily_return']], left_on='day', right_on='Date', how='inner')\n",
    "\n",
    "# Display the daily combined data\n",
    "print(\"Daily Combined Data:\\n\", daily_combined_data.head(10))\n",
    "\n",
    "# Handle missing data\n",
    "daily_combined_data.dropna(subset=['daily_return'], inplace=True)\n",
    "\n",
    "# Display the daily combined data after dropping NaNs\n",
    "print(\"Daily Combined Data After Dropping NaNs:\\n\", daily_combined_data[['day', 'sentiment', 'daily_return']].head(10))\n",
    "\n",
    "# Calculate daily correlation if there are enough data points\n",
    "if len(daily_combined_data) > 1:\n",
    "    daily_correlation = daily_combined_data['sentiment'].corr(daily_combined_data['daily_return'])\n",
    "    print(f'Correlation between daily sentiment and daily stock performance: {daily_correlation}')\n",
    "else:\n",
    "    print(\"Not enough data points to calculate daily correlation.\")\n",
    "'daily_return'], inplace=True)\n",
    "\n",
    "# Display the daily combined data after dropping NaNs\n",
    "print(\"Daily Combined Data After Dropping NaNs:\\n\", daily_combined_data[['day', 'sentiment', 'daily_return']].head(10))\n",
    "\n",
    "# Calculate daily correlation if there are enough data points\n",
    "if len(daily_combined_data) > 1:\n",
    "    daily_correlation = daily_combined_data['sentiment'].corr(daily_combined_data['daily_return'])\n",
    "    print(f'Correlation between daily sentiment and daily stock performance: {daily_correlation}')\n",
    "else:\n",
    "    print(\"Not enough data points to calculate daily correlation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
