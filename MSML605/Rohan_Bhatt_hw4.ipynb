{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1be9123c",
      "metadata": {
        "id": "1be9123c"
      },
      "source": [
        "\n",
        "# CUDA Programming Assignment (Using Numba on GPU)\n",
        "\n",
        "**Instructions:**\n",
        "- You will implement GPU kernels using Numba for:\n",
        "  - Vector Addition\n",
        "  - Dot Product\n",
        "  - ReLU Activation\n",
        "- Compare the performance and correctness against CPU implementations.\n",
        "\n",
        "Note: This assignment cannot be reliably executed on Google Colab due to compatibility issues between the Colab environment's Python 3.11, Numba, and CUDA toolkit versions.\n",
        "\n",
        "For successful execution, it's recommended to run this assignment on:\n",
        "\n",
        "- A local machine with a compatible NVIDIA GPU and environment\n",
        "\n",
        "OR\n",
        "\n",
        "- A Kaggle notebook with GPU enabled (https://www.kaggle.com/code)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3bd6805b",
      "metadata": {
        "id": "3bd6805b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available? False\n",
            "GPUs detected: <Managed Device 0>\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from numba import cuda, float32\n",
        "import math\n",
        "import time\n",
        "print(\"CUDA available?\", cuda.is_available())\n",
        "print(\"GPUs detected:\", cuda.gpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "499bce96",
      "metadata": {
        "id": "499bce96"
      },
      "outputs": [],
      "source": [
        "## Function for elementwise comparison between 2 arrays\n",
        "def compare(a, b, rtol=1e-5, atol=1e-8):\n",
        "    return np.allclose(a, b, rtol=rtol, atol=atol)\n",
        "\n",
        "## Function to check if the relative error (difference) between 2 values is within a defined threshold\n",
        "def within_relative_error(cpu_val, gpu_val, threshold=0.0002):\n",
        "    if cpu_val == 0:\n",
        "        return abs(gpu_val) < threshold\n",
        "    relative_error = abs(cpu_val - gpu_val) / abs(cpu_val)\n",
        "    return relative_error <= threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9ddc8127",
      "metadata": {
        "id": "9ddc8127"
      },
      "outputs": [],
      "source": [
        "## Function to compute dot product of 2 vectors using CPU\n",
        "def dot_product_cpu(A, B):\n",
        "    assert len(A) == len(B)\n",
        "    result = 0.0\n",
        "    for i in range(len(A)):\n",
        "        result += A[i] * B[i]\n",
        "    return result\n",
        "\n",
        "## Function to elementwise addition between 2 vectors using CPU\n",
        "def vector_add_cpu(A, B):\n",
        "    assert len(A) == len(B)\n",
        "    result = [0.0] * len(A)\n",
        "    for i in range(len(A)):\n",
        "        result[i] = A[i] + B[i]\n",
        "    return result\n",
        "\n",
        "## Function to apply ReLU activation on a vector using CPU\n",
        "def relu_activation_cpu(x):\n",
        "    return [val if val > 0 else 0 for val in x]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7828096d",
      "metadata": {
        "id": "7828096d"
      },
      "outputs": [],
      "source": [
        "## Number of datapoints\n",
        "N = 1_000_000\n",
        "\n",
        "## Randomly initializing the 2 vectors\n",
        "A = np.random.rand(N).astype(np.float32)\n",
        "B = np.random.rand(N).astype(np.float32)\n",
        "\n",
        "## Number of threads per block\n",
        "threads = 256\n",
        "## Number of required blocks\n",
        "blocks = math.ceil(N / threads)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cwCxknH9kGtQ",
      "metadata": {
        "id": "cwCxknH9kGtQ"
      },
      "outputs": [],
      "source": [
        "## Storing the data in the gpu for processing\n",
        "d_A = cuda.to_device(A)\n",
        "d_B = cuda.to_device(B)\n",
        "d_C = cuda.device_array_like(A)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33058656",
      "metadata": {
        "id": "33058656"
      },
      "source": [
        "## Part 1: Vector Addition (GPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "72b196cc",
      "metadata": {
        "id": "72b196cc"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "\n",
        "## Write a kernel function to perform vector addition between A and B\n",
        "\n",
        "#defining the GPU kernal\n",
        "@cuda.jit\n",
        "def vector_add_gpu(A, B, C):\n",
        "    i = cuda.grid(1)\n",
        "    if i < A.size:\n",
        "        C[i] = A[i] + B[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f12fc34a",
      "metadata": {
        "id": "f12fc34a"
      },
      "outputs": [
        {
          "ename": "NvvmSupportError",
          "evalue": "libNVVM cannot be found. Do `conda install cudatoolkit`:\nCould not find module 'nvvm.dll' (or one of its dependencies). Try using the full path with constructor syntax.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Rohan\\anaconda3\\lib\\site-packages\\numba\\cuda\\cudadrv\\nvvm.py\u001b[0m in \u001b[0;36m__new__\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m    118\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m                     \u001b[0minst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdriver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen_cudalib\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'nvvm'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Rohan\\anaconda3\\lib\\site-packages\\numba\\cuda\\cudadrv\\libs.py\u001b[0m in \u001b[0;36mopen_cudalib\u001b[1;34m(lib)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_cudalib\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Rohan\\anaconda3\\lib\\ctypes\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    382\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: Could not find module 'nvvm.dll' (or one of its dependencies). Try using the full path with constructor syntax.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mNvvmSupportError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-7-2482aee45cf5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mstart_gpu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mvector_add_gpu\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreads\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_A\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_B\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_C\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msynchronize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mgpu_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_gpu\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Rohan\\anaconda3\\lib\\site-packages\\numba\\cuda\\compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    819\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 821\u001b[1;33m         return self.dispatcher.call(args, self.griddim, self.blockdim,\n\u001b[0m\u001b[0;32m    822\u001b[0m                                     self.stream, self.sharedmem)\n\u001b[0;32m    823\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Rohan\\anaconda3\\lib\\site-packages\\numba\\cuda\\compiler.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, args, griddim, blockdim, stream, sharedmem)\u001b[0m\n\u001b[0;32m    962\u001b[0m             \u001b[0mkernel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moverloads\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 964\u001b[1;33m             \u001b[0mkernel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_dispatcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDispatcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m         \u001b[0mkernel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgriddim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblockdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msharedmem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Rohan\\anaconda3\\lib\\site-packages\\numba\\cuda\\compiler.py\u001b[0m in \u001b[0;36m_compile_for_args\u001b[1;34m(self, *args, **kws)\u001b[0m\n\u001b[0;32m    970\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mkws\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    971\u001b[0m         \u001b[0margtypes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypeof_pyval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 972\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margtypes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    973\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    974\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_search_new_conversions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Rohan\\anaconda3\\lib\\site-packages\\numba\\cuda\\compiler.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, sig)\u001b[0m\n\u001b[0;32m   1091\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_compile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1092\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Compilation disabled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1093\u001b[1;33m             kernel = _Kernel(self.py_func, argtypes, link=self.link,\n\u001b[0m\u001b[0;32m   1094\u001b[0m                              **self.targetoptions)\n\u001b[0;32m   1095\u001b[0m             \u001b[1;31m# Inspired by _DispatcherBase.add_overload, but differs slightly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Rohan\\anaconda3\\lib\\site-packages\\numba\\core\\compiler_lock.py\u001b[0m in \u001b[0;36m_acquire_compile_lock\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_acquire_compile_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_acquire_compile_lock\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Rohan\\anaconda3\\lib\\site-packages\\numba\\cuda\\compiler.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, py_func, argtypes, link, debug, inline, fastmath, extensions, max_registers, opt)\u001b[0m\n\u001b[0;32m    507\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextensions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextensions\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 509\u001b[1;33m         cres = compile_cuda(self.py_func, types.void, self.argtypes,\n\u001b[0m\u001b[0;32m    510\u001b[0m                             \u001b[0mdebug\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m                             inline=inline)\n",
            "\u001b[1;32mc:\\Users\\Rohan\\anaconda3\\lib\\site-packages\\numba\\core\\compiler_lock.py\u001b[0m in \u001b[0;36m_acquire_compile_lock\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_acquire_compile_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_acquire_compile_lock\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Rohan\\anaconda3\\lib\\site-packages\\numba\\cuda\\compiler.py\u001b[0m in \u001b[0;36mcompile_cuda\u001b[1;34m(pyfunc, return_type, args, debug, inline)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdescriptor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcuda_target\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mtypingctx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcuda_target\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypingctx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mtargetctx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcuda_target\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtargetctx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mflags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFlags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Rohan\\anaconda3\\lib\\site-packages\\numba\\cuda\\descriptor.py\u001b[0m in \u001b[0;36mtargetctx\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtargetctx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_targetctx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_targetctx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCUDATargetContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_typingctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_targetctx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Rohan\\anaconda3\\lib\\site-packages\\numba\\core\\base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, typing_context)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[1;31m# Initialize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Rohan\\anaconda3\\lib\\site-packages\\numba\\cuda\\target.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_internal_codegen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcodegen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mJITCUDACodegen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"numba.cuda.jit\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_target_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mll\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_target_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnvvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_data_layout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Rohan\\anaconda3\\lib\\site-packages\\numba\\core\\codegen.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1095\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_layout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1096\u001b[0m         self._llvm_module = ll.parse_assembly(\n\u001b[1;32m-> 1097\u001b[1;33m             str(self._create_empty_module(module_name)))\n\u001b[0m\u001b[0;32m   1098\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_llvm_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"global_codegen_module\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rtlinker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRuntimeLinker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Rohan\\anaconda3\\lib\\site-packages\\numba\\cuda\\codegen.py\u001b[0m in \u001b[0;36m_create_empty_module\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_layout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0mir_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_layout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_layout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0mnvvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_ir_version\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mir_module\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mir_module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Rohan\\anaconda3\\lib\\site-packages\\numba\\cuda\\cudadrv\\nvvm.py\u001b[0m in \u001b[0;36madd_ir_version\u001b[1;34m(mod)\u001b[0m\n\u001b[0;32m    909\u001b[0m     \u001b[1;34m\"\"\"Add NVVM IR version to module\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m     \u001b[0mi32\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mir\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIntType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 911\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mNVVM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_nvvm70\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    912\u001b[0m         \u001b[1;31m# NVVM IR 1.6, DWARF 3.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m         \u001b[0mir_versions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi32\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi32\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi32\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi32\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Rohan\\anaconda3\\lib\\site-packages\\numba\\cuda\\cudadrv\\nvvm.py\u001b[0m in \u001b[0;36m__new__\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m    122\u001b[0m                     errmsg = (\"libNVVM cannot be found. Do `conda install \"\n\u001b[0;32m    123\u001b[0m                               \"cudatoolkit`:\\n%s\")\n\u001b[1;32m--> 124\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mNvvmSupportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m                 \u001b[1;31m# Find & populate functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNvvmSupportError\u001b[0m: libNVVM cannot be found. Do `conda install cudatoolkit`:\nCould not find module 'nvvm.dll' (or one of its dependencies). Try using the full path with constructor syntax."
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "## NOTE: Run this cell twice — GPU kernel launch is slow on first run due to compilation.\n",
        "start_cpu = time.time()\n",
        "cpu_result = vector_add_cpu(A, B)\n",
        "cpu_time = (time.time() - start_cpu) * 1000\n",
        "\n",
        "start_gpu = time.time()\n",
        "vector_add_gpu[blocks, threads](d_A, d_B, d_C)\n",
        "cuda.synchronize()\n",
        "gpu_time = (time.time() - start_gpu) * 1000\n",
        "## Call the kernel function here to perform vector addition and generate the result\n",
        "print(f\"Vector Add - CPU Time: {cpu_time:.3f} ms\")\n",
        "print(f\"Vector Add - GPU Time: {gpu_time:.3f} ms\")\n",
        "# TODO\n",
        "## Call the 'compare' function to check if the cpu and gpu results are equal\n",
        "#bringing the result back to the host and verifying\n",
        "gpu_result = d_C.copy_to_host()\n",
        "print(f\"Match: {compare(cpu_result, gpu_result)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b773ad35",
      "metadata": {
        "id": "b773ad35"
      },
      "source": [
        "Example output:\n",
        "\n",
        "Vector Add - CPU Time: 239.820 ms\n",
        "\n",
        "Vector Add - GPU Time: 0.369 ms\n",
        "\n",
        "Match: True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "635ed6b4",
      "metadata": {
        "id": "635ed6b4"
      },
      "source": [
        "## Part 2: Dot Product (GPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fe1ddeb",
      "metadata": {
        "id": "0fe1ddeb"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "\n",
        "## Write a kernel function to perform dot product between A and B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b036e678",
      "metadata": {
        "id": "b036e678"
      },
      "outputs": [],
      "source": [
        "## NOTE: Run this cell twice — GPU kernel launch is slow on first run due to compilation.\n",
        "\n",
        "start_cpu = time.time()\n",
        "dot_cpu = dot_product_cpu(A, B)\n",
        "end_cpu = time.time()\n",
        "cpu_time = (end_cpu - start_cpu) * 1000\n",
        "\n",
        "start_gpu = time.time()\n",
        "\n",
        "# TODO\n",
        "## Call the kernel function here to perform dot product and generate the result\n",
        "\n",
        "cuda.synchronize()\n",
        "end_gpu = time.time()\n",
        "gpu_time = (end_gpu - start_gpu) * 1000\n",
        "\n",
        "print(f\"Dot Product - CPU Time: {cpu_time:.3f} ms\")\n",
        "print(f\"Dot Product - GPU Time: {gpu_time:.3f} ms\")\n",
        "\n",
        "# TODO\n",
        "## Call the 'within_relative_error' function to check if the cpu and gpu results are within the relative error"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d578cf68",
      "metadata": {
        "id": "d578cf68"
      },
      "source": [
        "Example output:\n",
        "\n",
        "Dot Product - CPU Time: 241.741 ms\n",
        "\n",
        "Dot Product - GPU Time: 0.244 ms\n",
        "\n",
        "Match: True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96b50fc4",
      "metadata": {
        "id": "96b50fc4"
      },
      "source": [
        "## Part 3: ReLU Activation (GPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b410534d",
      "metadata": {
        "id": "b410534d"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "\n",
        "## Write a kernel function to perform ReLU activation on A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "911f8ca8",
      "metadata": {
        "id": "911f8ca8"
      },
      "outputs": [],
      "source": [
        "## NOTE: Run this cell twice — GPU kernel launch is slow on first run due to compilation.\n",
        "\n",
        "start_cpu = time.time()\n",
        "relu_cpu = relu_activation_cpu(A)\n",
        "cpu_time = (time.time() - start_cpu) * 1000\n",
        "\n",
        "start_gpu = time.time()\n",
        "\n",
        "# TODO\n",
        "## Call the kernel function here to perform ReLU activation and generate the result\n",
        "\n",
        "cuda.synchronize()\n",
        "gpu_time = (time.time() - start_gpu) * 1000\n",
        "\n",
        "print(f\"ReLU Activation - CPU Time: {cpu_time:.3f} ms\")\n",
        "print(f\"ReLU Activation - GPU Time: {gpu_time:.3f} ms\")\n",
        "\n",
        "# TODO\n",
        "## Call the 'compare' function to check if the cpy and gpu results are equal"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "596a30d8",
      "metadata": {
        "id": "596a30d8"
      },
      "source": [
        "Example output:\n",
        "\n",
        "ReLU Activation - CPU Time: 116.852 ms\n",
        "\n",
        "ReLU Activation - GPU Time: 0.196 ms\n",
        "\n",
        "Match: True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "602a5827",
      "metadata": {
        "id": "602a5827"
      },
      "source": [
        "\n",
        "## Submission Instructions\n",
        "\n",
        "- Make sure **all outputs are printed clearly**.\n",
        "- Submit your completed `.ipynb` file on ELMS / Canvas."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
