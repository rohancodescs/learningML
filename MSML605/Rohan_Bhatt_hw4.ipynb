{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1be9123c",
      "metadata": {
        "id": "1be9123c"
      },
      "source": [
        "\n",
        "# CUDA Programming Assignment (Using Numba on GPU)\n",
        "\n",
        "**Instructions:**\n",
        "- You will implement GPU kernels using Numba for:\n",
        "  - Vector Addition\n",
        "  - Dot Product\n",
        "  - ReLU Activation\n",
        "- Compare the performance and correctness against CPU implementations.\n",
        "\n",
        "Note: This assignment cannot be reliably executed on Google Colab due to compatibility issues between the Colab environment's Python 3.11, Numba, and CUDA toolkit versions.\n",
        "\n",
        "For successful execution, it's recommended to run this assignment on:\n",
        "\n",
        "- A local machine with a compatible NVIDIA GPU and environment\n",
        "\n",
        "OR\n",
        "\n",
        "- A Kaggle notebook with GPU enabled (https://www.kaggle.com/code)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "3bd6805b",
      "metadata": {
        "id": "3bd6805b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available? True\n",
            "GPUs detected: <Managed Device 0>\n",
            "Using GPU: b'NVIDIA GeForce RTX 4060 Ti'\n",
            "GPU memory: (8, 9)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from numba import cuda, float32\n",
        "import math\n",
        "import time\n",
        "#checking if my GPU is being detected\n",
        "print(\"CUDA available?\", cuda.is_available())\n",
        "print(\"GPUs detected:\", cuda.gpus)\n",
        "dev = cuda.gpus[0]\n",
        "print(\"Using GPU:\", dev.name)\n",
        "print(\"GPU memory:\", dev.compute_capability)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "499bce96",
      "metadata": {
        "id": "499bce96"
      },
      "outputs": [],
      "source": [
        "## Function for elementwise comparison between 2 arrays\n",
        "def compare(a, b, rtol=1e-5, atol=1e-8):\n",
        "    return np.allclose(a, b, rtol=rtol, atol=atol)\n",
        "\n",
        "## Function to check if the relative error (difference) between 2 values is within a defined threshold\n",
        "def within_relative_error(cpu_val, gpu_val, threshold=0.0002):\n",
        "    if cpu_val == 0:\n",
        "        return abs(gpu_val) < threshold\n",
        "    relative_error = abs(cpu_val - gpu_val) / abs(cpu_val)\n",
        "    return relative_error <= threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "9ddc8127",
      "metadata": {
        "id": "9ddc8127"
      },
      "outputs": [],
      "source": [
        "## Function to compute dot product of 2 vectors using CPU\n",
        "def dot_product_cpu(A, B):\n",
        "    assert len(A) == len(B)\n",
        "    result = 0.0\n",
        "    for i in range(len(A)):\n",
        "        result += A[i] * B[i]\n",
        "    return result\n",
        "\n",
        "## Function to elementwise addition between 2 vectors using CPU\n",
        "def vector_add_cpu(A, B):\n",
        "    assert len(A) == len(B)\n",
        "    result = [0.0] * len(A)\n",
        "    for i in range(len(A)):\n",
        "        result[i] = A[i] + B[i]\n",
        "    return result\n",
        "\n",
        "## Function to apply ReLU activation on a vector using CPU\n",
        "def relu_activation_cpu(x):\n",
        "    return [val if val > 0 else 0 for val in x]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "7828096d",
      "metadata": {
        "id": "7828096d"
      },
      "outputs": [],
      "source": [
        "## Number of datapoints\n",
        "N = 1_000_000\n",
        "\n",
        "## Randomly initializing the 2 vectors\n",
        "A = np.random.rand(N).astype(np.float32)\n",
        "B = np.random.rand(N).astype(np.float32)\n",
        "\n",
        "## Number of threads per block\n",
        "threads = 256\n",
        "## Number of required blocks\n",
        "blocks = math.ceil(N / threads)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "cwCxknH9kGtQ",
      "metadata": {
        "id": "cwCxknH9kGtQ"
      },
      "outputs": [],
      "source": [
        "## Storing the data in the gpu for processing\n",
        "d_A = cuda.to_device(A)\n",
        "d_B = cuda.to_device(B)\n",
        "d_C = cuda.device_array_like(A)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33058656",
      "metadata": {
        "id": "33058656"
      },
      "source": [
        "## Part 1: Vector Addition (GPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "72b196cc",
      "metadata": {
        "id": "72b196cc"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "\n",
        "## Write a kernel function to perform vector addition between A and B\n",
        "\n",
        "#defining the GPU kernal\n",
        "@cuda.jit\n",
        "def vector_add_gpu(A, B, C):\n",
        "    i = cuda.grid(1)\n",
        "    if i < A.size:\n",
        "        C[i] = A[i] + B[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "f12fc34a",
      "metadata": {
        "id": "f12fc34a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------MY OUTPUT-------------------\n",
            "Vector Add - CPU Time: 225.049 ms\n",
            "Vector Add - GPU Time: 1.001 ms\n",
            "Match: True\n",
            "-------------------MY OUTPUT-------------------\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "## NOTE: Run this cell twice — GPU kernel launch is slow on first run due to compilation.\n",
        "start_cpu = time.time()\n",
        "cpu_result = vector_add_cpu(A, B)\n",
        "cpu_time = (time.time() - start_cpu) * 1000\n",
        "\n",
        "start_gpu = time.time()\n",
        "vector_add_gpu[blocks, threads](d_A, d_B, d_C)\n",
        "cuda.synchronize()\n",
        "gpu_time = (time.time() - start_gpu) * 1000\n",
        "## Call the kernel function here to perform vector addition and generate the result\n",
        "print(\"-------------------MY OUTPUT-------------------\")\n",
        "print(f\"Vector Add - CPU Time: {cpu_time:.3f} ms\")\n",
        "print(f\"Vector Add - GPU Time: {gpu_time:.3f} ms\")\n",
        "# TODO\n",
        "## Call the 'compare' function to check if the cpu and gpu results are equal\n",
        "#bringing the result back to the host and verifying\n",
        "gpu_result = d_C.copy_to_host()\n",
        "print(f\"Match: {compare(cpu_result, gpu_result)}\")\n",
        "print(\"-------------------MY OUTPUT-------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b773ad35",
      "metadata": {
        "id": "b773ad35"
      },
      "source": [
        "Example output:\n",
        "\n",
        "Vector Add - CPU Time: 239.820 ms\n",
        "\n",
        "Vector Add - GPU Time: 0.369 ms\n",
        "\n",
        "Match: True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "635ed6b4",
      "metadata": {
        "id": "635ed6b4"
      },
      "source": [
        "## Part 2: Dot Product (GPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "0fe1ddeb",
      "metadata": {
        "id": "0fe1ddeb"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "\n",
        "## Write a kernel function to perform dot product between A and B\n",
        "@cuda.jit\n",
        "def dot_product_gpu(A, B, partial_sums):\n",
        "    shared_mem = cuda.shared.array(256, dtype=float32) #shared memory array of 256 floats\n",
        "    i = cuda.grid(1) #getting the global thread index\n",
        "    thread_index = cuda.threadIdx.x #getting the thread index within the block\n",
        "    n = A.size #total number of elements in the input arrays\n",
        "\n",
        "    #each thread loading its product or 0.0 if out of bounds\n",
        "    if i < n:\n",
        "        shared_mem[thread_index] = A[i] * B[i] #element product and storing it in shared memory\n",
        "    else:\n",
        "        shared_mem[thread_index] = 0.0 #out of bounds, so store 0.0\n",
        "    cuda.syncthreads() #synchronize threads to make sure all writes to shared_mem are visible\n",
        "\n",
        "    #tree - based reduction\n",
        "    stride = cuda.blockDim.x // 2 #stride starts at half the number of threads in the block\n",
        "    while stride > 0:\n",
        "        if thread_index < stride:\n",
        "            shared_mem[thread_index] += shared_mem[thread_index + stride] #each threads add svalue from its partner\n",
        "        cuda.syncthreads() #sync after each step\n",
        "        stride //= 2 #halve the stride for the next iteration\n",
        "    if thread_index == 0: #only the first thread in the block writes the result to global memory\n",
        "        partial_sums[cuda.blockIdx.x] = shared_mem[0] #blocks total sum is now in shared_mem[0]\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "b036e678",
      "metadata": {
        "id": "b036e678"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------MY OUTPUT-------------------\n",
            "Dot Product - CPU Time: 312.018 ms\n",
            "Dot Product - GPU Time: 1.442 ms\n",
            "Match: True\n",
            "-------------------MY OUTPUT-------------------\n"
          ]
        }
      ],
      "source": [
        "## NOTE: Run this cell twice — GPU kernel launch is slow on first run due to compilation.\n",
        "\n",
        "start_cpu = time.time()\n",
        "dot_cpu = dot_product_cpu(A, B)\n",
        "end_cpu = time.time()\n",
        "cpu_time = (end_cpu - start_cpu) * 1000\n",
        "\n",
        "start_gpu = time.time()\n",
        "\n",
        "# TODO\n",
        "## Call the kernel function here to perform dot product and generate the result\n",
        "d_partial = cuda.device_array(blocks, dtype = np.float32) #allocating one float per block to hold partial sums\n",
        "dot_product_gpu[blocks, threads](d_A, d_B, d_partial)  # launch kernel with grid and block dimensions\n",
        "cuda.synchronize()\n",
        "end_gpu = time.time()\n",
        "gpu_time = (end_gpu - start_gpu) * 1000\n",
        "\n",
        "#getting partial sums from gpu to cpu and summing the partial results to  get the final dot product\n",
        "partial_host = d_partial.copy_to_host() #copying the partial sums from gpu to cpu\n",
        "dot_gpu = float(partial_host.sum()) #summing the partial results to get the final dot product\n",
        "print(\"-------------------MY OUTPUT-------------------\")\n",
        "print(f\"Dot Product - CPU Time: {cpu_time:.3f} ms\")\n",
        "print(f\"Dot Product - GPU Time: {gpu_time:.3f} ms\")\n",
        "# TODO\n",
        "## Call the 'within_relative_error' function to check if the cpu and gpu results are within the relative error\n",
        "match = within_relative_error(dot_cpu, dot_gpu)\n",
        "print(f\"Match: {match}\")\n",
        "print(\"-------------------MY OUTPUT-------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d578cf68",
      "metadata": {
        "id": "d578cf68"
      },
      "source": [
        "Example output:\n",
        "\n",
        "Dot Product - CPU Time: 241.741 ms\n",
        "\n",
        "Dot Product - GPU Time: 0.244 ms\n",
        "\n",
        "Match: True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96b50fc4",
      "metadata": {
        "id": "96b50fc4"
      },
      "source": [
        "## Part 3: ReLU Activation (GPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "b410534d",
      "metadata": {
        "id": "b410534d"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "\n",
        "## Write a kernel function to perform ReLU activation on A\n",
        "\n",
        "@cuda.jit\n",
        "def relu(x, out): #x is the input array, out is the output array\n",
        "    i = cuda.grid(1) #get global 1D thread index\n",
        "    if i < x.size: # only go on if the index is within array bounds\n",
        "        val = x[i] #get the value at the index\n",
        "        #apply ReLU activation function\n",
        "        if val > 0: #replacing negative values with 0\n",
        "            out[i] = val\n",
        "        else:\n",
        "            out[i] = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "911f8ca8",
      "metadata": {
        "id": "911f8ca8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------MY OUTPUT-------------------\n",
            "ReLU Activation - CPU Time: 1830.799 ms\n",
            "ReLU Activation - GPU Time: 2.000 ms\n",
            "Match: True\n",
            "-------------------MY OUTPUT-------------------\n"
          ]
        }
      ],
      "source": [
        "## NOTE: Run this cell twice — GPU kernel launch is slow on first run due to compilation.\n",
        "\n",
        "start_cpu = time.time()\n",
        "relu_cpu = relu_activation_cpu(A)\n",
        "cpu_time = (time.time() - start_cpu) * 1000\n",
        "\n",
        "start_gpu = time.time()\n",
        "\n",
        "# TODO\n",
        "## Call the kernel function here to perform ReLU activation and generate the result\n",
        "relu[blocks, threads](d_A, d_C) #launch kernel with grid and block dimensions\n",
        "cuda.synchronize() #wait for the kernel to finish\n",
        "relu_gpu = d_C.copy_to_host() #copying the result from gpu to cpu\n",
        "gpu_time = (time.time() - start_gpu) * 1000\n",
        "print(\"-------------------MY OUTPUT-------------------\")\n",
        "print(f\"ReLU Activation - CPU Time: {cpu_time:.3f} ms\")\n",
        "print(f\"ReLU Activation - GPU Time: {gpu_time:.3f} ms\")\n",
        "\n",
        "# TODO\n",
        "## Call the 'compare' function to check if the cpy and gpu results are equal\n",
        "match = compare(relu_cpu, relu_gpu) #comparing the results\n",
        "print(f\"Match: {match}\")\n",
        "print(\"-------------------MY OUTPUT-------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "596a30d8",
      "metadata": {
        "id": "596a30d8"
      },
      "source": [
        "Example output:\n",
        "\n",
        "ReLU Activation - CPU Time: 116.852 ms\n",
        "\n",
        "ReLU Activation - GPU Time: 0.196 ms\n",
        "\n",
        "Match: True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "602a5827",
      "metadata": {
        "id": "602a5827"
      },
      "source": [
        "\n",
        "## Submission Instructions\n",
        "\n",
        "- Make sure **all outputs are printed clearly**.\n",
        "- Submit your completed `.ipynb` file on ELMS / Canvas."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
