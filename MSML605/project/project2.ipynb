{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbbd1900",
   "metadata": {},
   "source": [
    "#### Rohan Bhatt, Shubhang Srikoti \n",
    "##### MSML605 -  Investigating the Impact of Storage Formats\n",
    "Problem statement: How does the choice of storage format (CSV, Parquet, HDF5) impact the overall performance of a machine learning pipeline and its processes (data ingestion, memory overhead, time-to-train, and more).\n",
    "\n",
    "NOTE: THIS NOTEBOOK IS THE 2GB VARIANT OF THE PARQUET\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4140169a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming download from 9063890944 bytes (592794126 bytes left)...\n",
      "Resuming download from https://www.kaggle.com/api/v1/datasets/download/jtbontinck/amex-parquet-file?dataset_version_number=1 (9063890944/9656685070) bytes left.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8.99G/8.99G [00:14<00:00, 41.6MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/rohan/.cache/kagglehub/datasets/jtbontinck/amex-parquet-file/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"jtbontinck/amex-parquet-file\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15d0980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keeping 169 row-groups  →  ~0.00 GB\n",
      "Subset rows: 16,895,213\n",
      "Wrote: data_2gb.parquet\n"
     ]
    }
   ],
   "source": [
    "#script to convert 10gb parquet to 2gb parquet\n",
    "import pyarrow.parquet as pq, pyarrow as pa, math\n",
    "from pathlib import Path\n",
    "\n",
    "SRC = Path(\"data.parquet\")          # 16-GB file\n",
    "DST = Path(\"data_2gb.parquet\")\n",
    "\n",
    "pq_src   = pq.ParquetFile(SRC)\n",
    "n_rg     = pq_src.num_row_groups\n",
    "\n",
    "# gather row-group sizes (compressed bytes on disk)\n",
    "rg_sizes = [pq_src.metadata.row_group(i).total_byte_size for i in range(n_rg)]\n",
    "\n",
    "target_bytes = 2 * 1024**3          # 2 GB\n",
    "keep_rg      = []\n",
    "cum          = 0\n",
    "for i, sz in enumerate(rg_sizes):\n",
    "    if cum + sz > target_bytes:\n",
    "        break\n",
    "    keep_rg.append(i)\n",
    "    cum += sz\n",
    "\n",
    "print(f\"Keeping {len(keep_rg)} row-groups  →  ~{cum/1024**3:.2f} GB\")\n",
    "\n",
    "# read & write subset\n",
    "tables = [pq_src.read_row_group(i) for i in keep_rg]\n",
    "subset = pa.concat_tables(tables)\n",
    "pq.write_table(subset, DST, compression=\"snappy\")   # or \"zstd\"\n",
    "\n",
    "print(f\"Subset rows: {subset.num_rows:,}\")\n",
    "print(\"Wrote:\", DST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38dfe004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in file: 16895213\n",
      "Columns in file: 193\n",
      "Schema: <pyarrow._parquet.ParquetSchema object at 0x1108bad40>\n",
      "required group field_id=-1 schema {\n",
      "  optional fixed_len_byte_array(16) field_id=-1 line_ID;\n",
      "  optional binary field_id=-1 customer_ID (String);\n",
      "  optional int64 field_id=-1 date (Timestamp(isAdjustedToUTC=false, timeUnit=microseconds, is_from_converted_type=false, force_set_converted_type=false));\n",
      "  optional float field_id=-1 P_2;\n",
      "  optional float field_id=-1 D_39;\n",
      "  optional float field_id=-1 B_1;\n",
      "  optional float field_id=-1 B_2;\n",
      "  optional float field_id=-1 R_1;\n",
      "  optional float field_id=-1 S_3;\n",
      "  optional float field_id=-1 D_41;\n",
      "  optional float field_id=-1 B_3;\n",
      "  optional float field_id=-1 D_42;\n",
      "  optional float field_id=-1 D_43;\n",
      "  optional float field_id=-1 D_44;\n",
      "  optional float field_id=-1 B_4;\n",
      "  optional float field_id=-1 D_45;\n",
      "  optional float field_id=-1 B_5;\n",
      "  optional float field_id=-1 R_2;\n",
      "  optional float field_id=-1 D_46;\n",
      "  optional float field_id=-1 D_47;\n",
      "  optional float field_id=-1 D_48;\n",
      "  optional float field_id=-1 D_49;\n",
      "  optional float field_id=-1 B_6;\n",
      "  optional float field_id=-1 B_7;\n",
      "  optional float field_id=-1 B_8;\n",
      "  optional float field_id=-1 D_50;\n",
      "  optional float field_id=-1 D_51;\n",
      "  optional float field_id=-1 B_9;\n",
      "  optional float field_id=-1 R_3;\n",
      "  optional float field_id=-1 D_52;\n",
      "  optional float field_id=-1 P_3;\n",
      "  optional float field_id=-1 B_10;\n",
      "  optional float field_id=-1 D_53;\n",
      "  optional float field_id=-1 S_5;\n",
      "  optional float field_id=-1 B_11;\n",
      "  optional float field_id=-1 S_6;\n",
      "  optional float field_id=-1 D_54;\n",
      "  optional float field_id=-1 R_4;\n",
      "  optional float field_id=-1 S_7;\n",
      "  optional float field_id=-1 B_12;\n",
      "  optional float field_id=-1 S_8;\n",
      "  optional float field_id=-1 D_55;\n",
      "  optional float field_id=-1 D_56;\n",
      "  optional float field_id=-1 B_13;\n",
      "  optional float field_id=-1 R_5;\n",
      "  optional float field_id=-1 D_58;\n",
      "  optional float field_id=-1 S_9;\n",
      "  optional float field_id=-1 B_14;\n",
      "  optional float field_id=-1 D_59;\n",
      "  optional float field_id=-1 D_60;\n",
      "  optional float field_id=-1 D_61;\n",
      "  optional float field_id=-1 B_15;\n",
      "  optional float field_id=-1 S_11;\n",
      "  optional float field_id=-1 D_62;\n",
      "  optional binary field_id=-1 D_63 (String);\n",
      "  optional binary field_id=-1 D_64 (String);\n",
      "  optional float field_id=-1 D_65;\n",
      "  optional float field_id=-1 B_16;\n",
      "  optional float field_id=-1 B_17;\n",
      "  optional float field_id=-1 B_18;\n",
      "  optional float field_id=-1 B_19;\n",
      "  optional float field_id=-1 D_66;\n",
      "  optional float field_id=-1 B_20;\n",
      "  optional float field_id=-1 D_68;\n",
      "  optional float field_id=-1 S_12;\n",
      "  optional float field_id=-1 R_6;\n",
      "  optional float field_id=-1 S_13;\n",
      "  optional float field_id=-1 B_21;\n",
      "  optional float field_id=-1 D_69;\n",
      "  optional float field_id=-1 B_22;\n",
      "  optional float field_id=-1 D_70;\n",
      "  optional float field_id=-1 D_71;\n",
      "  optional float field_id=-1 D_72;\n",
      "  optional float field_id=-1 S_15;\n",
      "  optional float field_id=-1 B_23;\n",
      "  optional float field_id=-1 D_73;\n",
      "  optional float field_id=-1 P_4;\n",
      "  optional float field_id=-1 D_74;\n",
      "  optional float field_id=-1 D_75;\n",
      "  optional float field_id=-1 D_76;\n",
      "  optional float field_id=-1 B_24;\n",
      "  optional float field_id=-1 R_7;\n",
      "  optional float field_id=-1 D_77;\n",
      "  optional float field_id=-1 B_25;\n",
      "  optional float field_id=-1 B_26;\n",
      "  optional float field_id=-1 D_78;\n",
      "  optional float field_id=-1 D_79;\n",
      "  optional float field_id=-1 R_8;\n",
      "  optional float field_id=-1 R_9;\n",
      "  optional float field_id=-1 S_16;\n",
      "  optional float field_id=-1 D_80;\n",
      "  optional float field_id=-1 R_10;\n",
      "  optional float field_id=-1 R_11;\n",
      "  optional float field_id=-1 B_27;\n",
      "  optional float field_id=-1 D_81;\n",
      "  optional float field_id=-1 D_82;\n",
      "  optional float field_id=-1 S_17;\n",
      "  optional float field_id=-1 R_12;\n",
      "  optional float field_id=-1 B_28;\n",
      "  optional float field_id=-1 R_13;\n",
      "  optional float field_id=-1 D_83;\n",
      "  optional float field_id=-1 R_14;\n",
      "  optional float field_id=-1 R_15;\n",
      "  optional float field_id=-1 D_84;\n",
      "  optional float field_id=-1 R_16;\n",
      "  optional float field_id=-1 B_29;\n",
      "  optional float field_id=-1 B_30;\n",
      "  optional float field_id=-1 S_18;\n",
      "  optional float field_id=-1 D_86;\n",
      "  optional float field_id=-1 D_87;\n",
      "  optional float field_id=-1 R_17;\n",
      "  optional float field_id=-1 R_18;\n",
      "  optional float field_id=-1 D_88;\n",
      "  optional int64 field_id=-1 B_31;\n",
      "  optional float field_id=-1 S_19;\n",
      "  optional float field_id=-1 R_19;\n",
      "  optional float field_id=-1 B_32;\n",
      "  optional float field_id=-1 S_20;\n",
      "  optional float field_id=-1 R_20;\n",
      "  optional float field_id=-1 R_21;\n",
      "  optional float field_id=-1 B_33;\n",
      "  optional float field_id=-1 D_89;\n",
      "  optional float field_id=-1 R_22;\n",
      "  optional float field_id=-1 R_23;\n",
      "  optional float field_id=-1 D_91;\n",
      "  optional float field_id=-1 D_92;\n",
      "  optional float field_id=-1 D_93;\n",
      "  optional float field_id=-1 D_94;\n",
      "  optional float field_id=-1 R_24;\n",
      "  optional float field_id=-1 R_25;\n",
      "  optional float field_id=-1 D_96;\n",
      "  optional float field_id=-1 S_22;\n",
      "  optional float field_id=-1 S_23;\n",
      "  optional float field_id=-1 S_24;\n",
      "  optional float field_id=-1 S_25;\n",
      "  optional float field_id=-1 S_26;\n",
      "  optional float field_id=-1 D_102;\n",
      "  optional float field_id=-1 D_103;\n",
      "  optional float field_id=-1 D_104;\n",
      "  optional float field_id=-1 D_105;\n",
      "  optional float field_id=-1 D_106;\n",
      "  optional float field_id=-1 D_107;\n",
      "  optional float field_id=-1 B_36;\n",
      "  optional float field_id=-1 B_37;\n",
      "  optional float field_id=-1 R_26;\n",
      "  optional float field_id=-1 R_27;\n",
      "  optional float field_id=-1 B_38;\n",
      "  optional float field_id=-1 D_108;\n",
      "  optional float field_id=-1 D_109;\n",
      "  optional float field_id=-1 D_110;\n",
      "  optional float field_id=-1 D_111;\n",
      "  optional float field_id=-1 B_39;\n",
      "  optional float field_id=-1 D_112;\n",
      "  optional float field_id=-1 B_40;\n",
      "  optional float field_id=-1 S_27;\n",
      "  optional float field_id=-1 D_113;\n",
      "  optional float field_id=-1 D_114;\n",
      "  optional float field_id=-1 D_115;\n",
      "  optional float field_id=-1 D_116;\n",
      "  optional float field_id=-1 D_117;\n",
      "  optional float field_id=-1 D_118;\n",
      "  optional float field_id=-1 D_119;\n",
      "  optional float field_id=-1 D_120;\n",
      "  optional float field_id=-1 D_121;\n",
      "  optional float field_id=-1 D_122;\n",
      "  optional float field_id=-1 D_123;\n",
      "  optional float field_id=-1 D_124;\n",
      "  optional float field_id=-1 D_125;\n",
      "  optional float field_id=-1 D_126;\n",
      "  optional float field_id=-1 D_127;\n",
      "  optional float field_id=-1 D_128;\n",
      "  optional float field_id=-1 D_129;\n",
      "  optional float field_id=-1 B_41;\n",
      "  optional float field_id=-1 B_42;\n",
      "  optional float field_id=-1 D_130;\n",
      "  optional float field_id=-1 D_131;\n",
      "  optional float field_id=-1 D_132;\n",
      "  optional float field_id=-1 D_133;\n",
      "  optional float field_id=-1 R_28;\n",
      "  optional float field_id=-1 D_134;\n",
      "  optional float field_id=-1 D_135;\n",
      "  optional float field_id=-1 D_136;\n",
      "  optional float field_id=-1 D_137;\n",
      "  optional float field_id=-1 D_138;\n",
      "  optional float field_id=-1 D_139;\n",
      "  optional float field_id=-1 D_140;\n",
      "  optional float field_id=-1 D_141;\n",
      "  optional float field_id=-1 D_142;\n",
      "  optional float field_id=-1 D_143;\n",
      "  optional float field_id=-1 D_144;\n",
      "  optional float field_id=-1 D_145;\n",
      "  optional int32 field_id=-1 target (Int(bitWidth=8, isSigned=true));\n",
      "  optional int32 field_id=-1 test (Int(bitWidth=8, isSigned=true));\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sanity check of parquet file\n",
    "pq_file = pq.ParquetFile(\"data_2gb.parquet\")\n",
    "print(\"Rows in file:\", pq_file.metadata.num_rows)\n",
    "print(\"Columns in file:\", pq_file.metadata.num_columns)\n",
    "print(\"Schema:\", pq_file.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1723e5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "# import tables # for hdf5\n",
    "import time, datetime, os, psutil\n",
    "import xgboost as xgb\n",
    "from pathlib import Path, PureWindowsPath\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cc8fc3",
   "metadata": {},
   "source": [
    "Converting 2Gb parquet -> csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a30a1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row groups in file: 17\n",
      "row-group 1/17 appended\n",
      "row-group 2/17 appended\n",
      "row-group 3/17 appended\n",
      "row-group 4/17 appended\n",
      "row-group 5/17 appended\n",
      "row-group 6/17 appended\n",
      "row-group 7/17 appended\n",
      "row-group 8/17 appended\n",
      "row-group 9/17 appended\n",
      "row-group 10/17 appended\n",
      "row-group 11/17 appended\n",
      "row-group 12/17 appended\n",
      "row-group 13/17 appended\n",
      "row-group 14/17 appended\n",
      "row-group 15/17 appended\n",
      "row-group 16/17 appended\n",
      "row-group 17/17 appended\n",
      "All done → data.csv\n"
     ]
    }
   ],
   "source": [
    "# in/out file paths\n",
    "IN_FILE  = Path(\"data_2gb.parquet\")\n",
    "OUT_CSV  = Path(\"data.csv\") # final single file\n",
    "\n",
    "#opening the parquet file\n",
    "pq_file = pq.ParquetFile(IN_FILE, memory_map=True)\n",
    "n_rg = pq_file.num_row_groups\n",
    "print(f\"Row groups in file: {n_rg}\")\n",
    "\n",
    "# write loop\n",
    "first_chunk = True\n",
    "for rg in range(n_rg):\n",
    "    # load one row group into Arrow Table (stays off heap)\n",
    "    table = pq_file.read_row_group(rg)\n",
    "    # convert to pandas\n",
    "    df = table.to_pandas(types_mapper=pd.ArrowDtype)\n",
    "    # write / append\n",
    "    if first_chunk:\n",
    "        df.to_csv(OUT_CSV, index=False, mode=\"w\", header=True)\n",
    "        first_chunk = False\n",
    "    else:\n",
    "        df.to_csv(OUT_CSV, index=False, mode=\"a\", header=False)\n",
    "    \n",
    "    # free memory\n",
    "    del df, table\n",
    "    gc.collect()\n",
    "    print(f\"row-group {rg+1}/{n_rg} appended\")\n",
    "\n",
    "print(\"All done →\", OUT_CSV) #10 min - 23 rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e6f3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             line_ID  \\\n",
      "0  b'\\xb6a\\x82\\x86f#F\\x1d\\x8c\\x94\\x7f\\x8d\\x944\\xd...   \n",
      "1        b'L\\xa8+-\\xa8\\x8dM\\xa9\\x96g\\xed0I\\x95\\x1e$'   \n",
      "2        b']s_\\x87\\xaf B\\xec\\xbeEg\\xb5\\x1e\\xb2\\xaed'   \n",
      "3     b'\\xfb^\\xd4{Q\\xb5HO\\xa8\\xb6\\xf6\\xca\\xb1]@\\x99'   \n",
      "4  b'`\\xa5\\x96\\xf6\\x1b\\rG\\x8d\\xab\\\\\\x16\\x8d\\xe1\\x...   \n",
      "\n",
      "                                         customer_ID       date       P_2  \\\n",
      "0  d00b98b2401d26197fa1d6102cdc1c9bbed7c066b8aaa9... 2018-03-06  0.366254   \n",
      "1  d00bc5e66e3aac9eae7c9e94621b36d196566d61ef7a32... 2018-03-25  0.312623   \n",
      "2  d00bd125cf6fa463a6c57b9959b8a4197f6f79fb154fee... 2018-03-28  0.395606   \n",
      "3  d00bfbdee3081206258a4b4fb2ef2eb311697f37056bfb... 2018-03-01  0.977543   \n",
      "4  d00c0dd295ada176c4e697d4cc1cd2f0d572870f770859... 2018-03-26  0.934237   \n",
      "\n",
      "       D_39       B_1       B_2       R_1       S_3      D_41  ...     D_138  \\\n",
      "0  0.003860  0.009151  0.818901  0.008979  0.143153  0.005497  ...  0.500092   \n",
      "1  0.179014  0.560108  0.029272  0.756391  0.091940  0.005489  ...       NaN   \n",
      "2  1.066026  0.731072  0.019496  0.751631  0.728473  0.574862  ...       NaN   \n",
      "3  0.299848  0.016523  1.002691  0.009632  0.107092  0.003081  ...       NaN   \n",
      "4  0.003405  0.295735  1.001624  0.005357  0.142128  0.000591  ...       NaN   \n",
      "\n",
      "      D_139     D_140     D_141     D_142     D_143     D_144     D_145  \\\n",
      "0  0.004264  0.009833  0.000985       NaN  0.006703  0.005501  0.005352   \n",
      "1  0.005151  0.005813  0.007565       NaN  0.000990  0.001063  0.008251   \n",
      "2  1.008811  0.001852  0.964900  0.478015  1.001196  0.173963  0.091996   \n",
      "3  0.006636  0.003483  0.005275       NaN  0.009845  0.009467  0.008976   \n",
      "4  1.004110  0.002928  1.086800  1.076618  1.001960  1.003030  0.097089   \n",
      "\n",
      "   target  test  \n",
      "0     0.0     0  \n",
      "1     1.0     0  \n",
      "2     1.0     0  \n",
      "3     0.0     0  \n",
      "4     0.0     0  \n",
      "\n",
      "[5 rows x 193 columns]\n",
      "                                             line_ID  \\\n",
      "0  b'\\xb6a\\x82\\x86f#F\\x1d\\x8c\\x94\\x7f\\x8d\\x944\\xd...   \n",
      "1        b'L\\xa8+-\\xa8\\x8dM\\xa9\\x96g\\xed0I\\x95\\x1e$'   \n",
      "2        b']s_\\x87\\xaf B\\xec\\xbeEg\\xb5\\x1e\\xb2\\xaed'   \n",
      "3     b'\\xfb^\\xd4{Q\\xb5HO\\xa8\\xb6\\xf6\\xca\\xb1]@\\x99'   \n",
      "4  b'`\\xa5\\x96\\xf6\\x1b\\rG\\x8d\\xab\\\\\\x16\\x8d\\xe1\\x...   \n",
      "\n",
      "                                         customer_ID                 date  \\\n",
      "0  d00b98b2401d26197fa1d6102cdc1c9bbed7c066b8aaa9...  2018-03-06 00:00:00   \n",
      "1  d00bc5e66e3aac9eae7c9e94621b36d196566d61ef7a32...  2018-03-25 00:00:00   \n",
      "2  d00bd125cf6fa463a6c57b9959b8a4197f6f79fb154fee...  2018-03-28 00:00:00   \n",
      "3  d00bfbdee3081206258a4b4fb2ef2eb311697f37056bfb...  2018-03-01 00:00:00   \n",
      "4  d00c0dd295ada176c4e697d4cc1cd2f0d572870f770859...  2018-03-26 00:00:00   \n",
      "\n",
      "        P_2      D_39       B_1       B_2       R_1       S_3      D_41  ...  \\\n",
      "0  0.366254  0.003860  0.009151  0.818901  0.008979  0.143153  0.005497  ...   \n",
      "1  0.312623  0.179014  0.560108  0.029272  0.756391  0.091940  0.005489  ...   \n",
      "2  0.395606  1.066026  0.731072  0.019496  0.751631  0.728473  0.574862  ...   \n",
      "3  0.977543  0.299848  0.016523  1.002691  0.009632  0.107092  0.003081  ...   \n",
      "4  0.934237  0.003405  0.295735  1.001624  0.005357  0.142128  0.000591  ...   \n",
      "\n",
      "      D_138     D_139     D_140     D_141     D_142     D_143     D_144  \\\n",
      "0  0.500092  0.004264  0.009833  0.000985       NaN  0.006703  0.005501   \n",
      "1       NaN  0.005151  0.005813  0.007565       NaN  0.000990  0.001063   \n",
      "2       NaN  1.008811  0.001852  0.964900  0.478015  1.001196  0.173963   \n",
      "3       NaN  0.006636  0.003483  0.005275       NaN  0.009845  0.009467   \n",
      "4       NaN  1.004110  0.002928  1.086800  1.076618  1.001960  1.003030   \n",
      "\n",
      "      D_145  target  test  \n",
      "0  0.005352       0     0  \n",
      "1  0.008251       1     0  \n",
      "2  0.091996       1     0  \n",
      "3  0.008976       0     0  \n",
      "4  0.097089       0     0  \n",
      "\n",
      "[5 rows x 193 columns]\n"
     ]
    }
   ],
   "source": [
    "#verifying both files before hdf5 conversion\n",
    "df_pq = pq.read_table(\"data_2gb.parquet\").to_pandas().head(5)\n",
    "df_csv = pd.read_csv(\"data.csv\", nrows=5)\n",
    "\n",
    "print(df_pq.head())\n",
    "print(df_csv.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6434c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy : 1.24.4\n",
      "PyTables : 3.9.2\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import tables\n",
    "print(\"NumPy :\", numpy.__version__)\n",
    "print(\"PyTables :\", tables.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58c7494d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ row-group 1/17  |  elapsed 0.4 min\n",
      "✓ row-group 2/17  |  elapsed 2.7 min\n",
      "✓ row-group 3/17  |  elapsed 5.4 min\n",
      "✓ row-group 4/17  |  elapsed 7.6 min\n",
      "✓ row-group 5/17  |  elapsed 13.1 min\n",
      "✓ row-group 6/17  |  elapsed 71.3 min\n",
      "✓ row-group 7/17  |  elapsed 73.4 min\n",
      "✓ row-group 8/17  |  elapsed 75.6 min\n",
      "✓ row-group 9/17  |  elapsed 77.8 min\n",
      "✓ row-group 10/17  |  elapsed 230.3 min\n",
      "✓ row-group 11/17  |  elapsed 335.3 min\n",
      "✓ row-group 12/17  |  elapsed 361.5 min\n",
      "✓ row-group 13/17  |  elapsed 363.7 min\n",
      "✓ row-group 14/17  |  elapsed 366.0 min\n",
      "✓ row-group 15/17  |  elapsed 368.3 min\n",
      "✓ row-group 16/17  |  elapsed 370.8 min\n",
      "✓ row-group 17/17  |  elapsed 373.0 min\n",
      "\n",
      "Parquet → HDF5 completed in 373.0 minutes\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq, pandas as pd, numpy as np, gc, time\n",
    "from pathlib import Path\n",
    "\n",
    "IN_PARQUET = Path(\"data_2gb.parquet\")\n",
    "OUT_H5     = Path(\"data_2gb.h5\")\n",
    "\n",
    "pq_file = pq.ParquetFile(IN_PARQUET, memory_map=True)\n",
    "n_rg    = pq_file.num_row_groups\n",
    "\n",
    "t0 = time.time()\n",
    "with pd.HDFStore(OUT_H5, \"w\", complib=\"zlib\", complevel=6) as store:\n",
    "    for i in range(n_rg):\n",
    "        df = pq_file.read_row_group(i).to_pandas()\n",
    "\n",
    "        # bytes → hex-strings\n",
    "        for col in df.select_dtypes(\"object\"):\n",
    "            if isinstance(df[col].iloc[0], (bytes, bytearray)):\n",
    "                df[col] = df[col].apply(lambda b: b.hex())\n",
    "\n",
    "        # force consistent NumPy int8 for label columns\n",
    "        for col in [\"target\", \"test\"]:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna(-1).astype(np.int8)\n",
    "\n",
    "        store.append(\"train\", df, data_columns=True, index=False)\n",
    "        del df; gc.collect()\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"✓ row-group {i+1}/{n_rg}  |  elapsed {elapsed/60:.1f} min\")\n",
    "\n",
    "total = time.time() - t0\n",
    "print(f\"\\nParquet → HDF5 completed in {total/60:.1f} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40dec8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            line_ID  \\\n",
      "0  b66182866623461d8c947f8d9434d7b6   \n",
      "1  4ca82b2da88d4da99667ed3049951e24   \n",
      "2  5d735f87af2042ecbe4567b51eb2ae64   \n",
      "3  fb5ed47b51b5484fa8b6f6cab15d4099   \n",
      "4  60a596f61b0d478dab5c168de1c6f6be   \n",
      "\n",
      "                                         customer_ID                    date  \\\n",
      "0  d00b98b2401d26197fa1d6102cdc1c9bbed7c066b8aaa9... 1970-01-18 14:18:14.400   \n",
      "1  d00bc5e66e3aac9eae7c9e94621b36d196566d61ef7a32... 1970-01-18 14:45:36.000   \n",
      "2  d00bd125cf6fa463a6c57b9959b8a4197f6f79fb154fee... 1970-01-18 14:49:55.200   \n",
      "3  d00bfbdee3081206258a4b4fb2ef2eb311697f37056bfb... 1970-01-18 14:11:02.400   \n",
      "4  d00c0dd295ada176c4e697d4cc1cd2f0d572870f770859... 1970-01-18 14:47:02.400   \n",
      "\n",
      "        P_2      D_39       B_1       B_2       R_1       S_3      D_41  ...  \\\n",
      "0  0.366254  0.003860  0.009151  0.818901  0.008979  0.143153  0.005497  ...   \n",
      "1  0.312623  0.179014  0.560108  0.029272  0.756391  0.091940  0.005489  ...   \n",
      "2  0.395606  1.066026  0.731072  0.019496  0.751631  0.728473  0.574862  ...   \n",
      "3  0.977543  0.299848  0.016523  1.002691  0.009632  0.107092  0.003081  ...   \n",
      "4  0.934237  0.003405  0.295735  1.001624  0.005357  0.142128  0.000591  ...   \n",
      "\n",
      "      D_138     D_139     D_140     D_141     D_142     D_143     D_144  \\\n",
      "0  0.500092  0.004264  0.009833  0.000985       NaN  0.006703  0.005501   \n",
      "1       NaN  0.005151  0.005813  0.007565       NaN  0.000990  0.001063   \n",
      "2       NaN  1.008811  0.001852  0.964900  0.478015  1.001196  0.173963   \n",
      "3       NaN  0.006636  0.003483  0.005275       NaN  0.009845  0.009467   \n",
      "4       NaN  1.004110  0.002928  1.086800  1.076618  1.001960  1.003030   \n",
      "\n",
      "      D_145  target  test  \n",
      "0  0.005352       0     0  \n",
      "1  0.008251       1     0  \n",
      "2  0.091996       1     0  \n",
      "3  0.008976       0     0  \n",
      "4  0.097089       0     0  \n",
      "\n",
      "[5 rows x 193 columns]\n",
      "\n",
      "HDF5 quick read time: 0.07877683639526367 sec\n",
      "HDF5 size on disk: 8.900587243959308 GB\n"
     ]
    }
   ],
   "source": [
    "#sanity check of hdf5 file\n",
    "import pandas as pd, os, time\n",
    "\n",
    "start = time.time()\n",
    "df_head = pd.read_hdf(\"data_2gb.h5\", key=\"train\", stop=5)\n",
    "print(df_head.head())\n",
    "print(\"\\nHDF5 quick read time:\", time.time()-start, \"sec\")\n",
    "print(\"HDF5 size on disk:\", os.path.getsize(\"data_2gb.h5\")/1024**3, \"GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc922ff",
   "metadata": {},
   "source": [
    "##### Actual benchmarks of each format\n",
    "(now that we've converted each file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d7d1fe0",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import psutil, time, pandas as pd, pyarrow.parquet as pq\n",
    "import xgboost as xgb\n",
    "import csv\n",
    "from pathlib import Path\n",
    "RESULT_FILE = \"benchmark_results.csv\"\n",
    "\n",
    "def load_csv():\n",
    "    return pd.read_csv(\"data.csv\")\n",
    "\n",
    "def load_parquet():\n",
    "    return pq.read_table(\"data_2gb.parquet\").to_pandas()\n",
    "\n",
    "def load_hdf5():\n",
    "    return pd.read_hdf(\"data_2gb.h5\", key=\"train\")\n",
    "\n",
    "def log_result(record):\n",
    "    header = record.keys()\n",
    "    write_header = not Path(RESULT_FILE).exists()\n",
    "    with open(RESULT_FILE, \"a\", newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=header)\n",
    "        if write_header:\n",
    "            w.writeheader()\n",
    "        w.writerow(record)\n",
    "\n",
    "def run_case(tag, loader):\n",
    "    proc = psutil.Process()\n",
    "    mem0 = proc.memory_info().rss\n",
    "    t0   = time.perf_counter()\n",
    "    df   = loader()\n",
    "    load_sec = time.perf_counter() - t0\n",
    "    mem_peak = proc.memory_info().rss - mem0\n",
    "\n",
    "    # very light preprocessing example (fillna 0)\n",
    "    X = df.drop(columns=[\"target\", \"test\"])\n",
    "    y = df[\"target\"].fillna(0).astype(\"int8\")\n",
    "\n",
    "    dtrain = xgb.DMatrix(X, label=y)\n",
    "    t1 = time.perf_counter()\n",
    "    xgb.train({\"objective\":\"binary:logistic\", \"tree_method\":\"hist\", \"nthread\": 4},\n",
    "              dtrain, num_boost_round=50)\n",
    "    train_sec = time.perf_counter() - t1\n",
    "    log_result({\"tag\": tag, \"load_sec\": load_sec, \"mem_peak\": mem_peak/1024**3, \"train_sec\": train_sec})\n",
    "    return tag, load_sec, mem_peak/1024**3, train_sec\n",
    "for tag, loader in [(\"CSV\", load_csv),\n",
    "                    (\"Parquet\", load_parquet),\n",
    "                    (\"HDF5\", load_hdf5)]:\n",
    "    tag, load_s, mem_gb, train_s = run_case(tag, loader)\n",
    "    display({\"format\": tag, \"load\": load_s, \"train\": train_s, \"peakGB\": mem_gb})\n",
    "    time.sleep(2)          # brief pause so macOS frees RAM\n",
    "\n",
    "# results = [run_case(\"CSV\", load_csv),\n",
    "#            run_case(\"Parquet\", load_parquet),\n",
    "#            run_case(\"HDF5\", load_hdf5)]\n",
    "# for r in results:\n",
    "#     print(f\"{r[0]:7s}  load={r[1]:6.1f}s  peakRAM={r[2]:4.1f} GB  train={r[3]:5.1f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b94a523",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hdf5env)",
   "language": "python",
   "name": "hdf5env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
